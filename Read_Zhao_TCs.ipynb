{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook to read the Zhao tracker-derived TC data \n",
    "#saved from Jeff's model runs, \n",
    "#and save them in an XArray dataset/\n",
    "#NetCDF file in a simlar format to the IBTrACS data\n",
    "#(Jeff used Matlab cell structures)\n",
    "\n",
    "#(As of 9-25-20 only have read permissions for V1, not V2)\n",
    "#(Or: could I just modify and re-run Jeff's Matlab code?)\n",
    "#(Can only run in command line, and aggravation of Matlab for plots etc.\n",
    "# would outweigh short time spent rewriting algorithms)\n",
    "\n",
    "#10-15-20: adding wind in knots, 0-360 longitude, and regional flags\n",
    "#10-22-20: adding specific tag for SH rather than relying on not NH, which doesn't work well with the nans; \n",
    "#          editing basin boundaries; and adding genesis_region variable for each storm\n",
    "#11-5-20: adding variable for the season in the Southern Hemisphere, from July to June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__\n",
    "#1.16.3\n",
    "#The datetime is far from stable in this version.\n",
    "#Just work with year, month, day, hour for now--can always add a datetime variable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the files for the V1 TCs\n",
    "#Dict with entry for each year\n",
    "dict_files_v1 = dict()\n",
    "for year in (np.arange(21)+1980):\n",
    "    with open('Zhao_TCs_v1/traj_'+str(year)) as f:\n",
    "        dict_files_v1[year] = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for V2\n",
    "dict_files_v2 = dict()\n",
    "for year in (np.arange(21)+1980):\n",
    "    with open('Zhao_TCs_v2/traj_'+str(year)) as f:\n",
    "        dict_files_v2[year] = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_files_v1[1980]\n",
    "\n",
    "# ['start    15  1980     1     1     6\\n',\n",
    "#  '  164.25  -18.25   15.80    9.85  1980     1     1     6\\n',\n",
    "#  '  164.25  -18.75   17.27    9.84  1980     1     1    12\\n',\n",
    "#  '  164.25  -19.25   16.72    9.82  1980     1     1    18\\n',\n",
    "#  '  164.25  -19.25   17.28    9.84  1980     1     2     0\\n',\n",
    "#  '  164.25  -19.75   20.29    9.81  1980     1     2     6\\n',\n",
    "#  '  164.75  -19.75   19.68    9.84  1980     1     2    12\\n',\n",
    "#  '  165.25  -19.75   19.87    9.84  1980     1     2    18\\n',\n",
    "#  '  165.75  -19.75   16.63    9.86  1980     1     3     0\\n',\n",
    "#  '  166.25  -20.25   15.55    9.86  1980     1     3     6\\n',\n",
    "#  '  166.75  -20.25   16.21    9.89  1980     1     3    12\\n',\n",
    "#  '  167.75  -20.25   14.36    9.89  1980     1     3    18\\n',\n",
    "#  '  168.75  -20.25   13.76    9.91  1980     1     4     0\\n',\n",
    "#  '  169.25  -20.75   14.35    9.89  1980     1     4     6\\n',\n",
    "#  '  170.25  -20.75   15.66    9.90  1980     1     4    12\\n',\n",
    "#  '  171.25  -21.25   16.27    9.89  1980     1     4    18\\n',\n",
    "\n",
    "#etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now the harder part: how to read them and structure the resulting file?\n",
    "IBTrACS is a NetCDF file on a 2D grid with dimensions storm and date_time, \n",
    "with time, lat, lon as coordinates that are not dimensions \n",
    "(or the other way around?)\n",
    "Different variables are combined in one dataset.\n",
    "\n",
    "Need to loop through the entire file and each storm somehow \n",
    "Ultimately concatenate all the years into one file. \n",
    "\n",
    "Files are tab delimeted... that doesn't seem to have followed through to \n",
    "the reading?\n",
    "\n",
    "Format of the storm header:\n",
    "\"start\", duration (# of lines), start year, start month, start day, start hour\n",
    "\n",
    "Format of each line after the storm header: \n",
    "lon, lat, wind speed, surface pressure, year, month, day, hour\n",
    "\n",
    "(Should probably combine the last 4 into a single datetime)\n",
    "(All times UTC I assume)\n",
    "\n",
    "Can get the last 4 header entries from the 1st line so don't read from header.\n",
    "\n",
    "So could have an if statement for if the first line is start, \n",
    "then another loop depending on the duration field. \n",
    "\n",
    "Just like IBTrACS, put everything in arrays 360 wide, padding with nans--\n",
    "fill one line of the array for each storm, one array for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read in storm from a dict of files,\n",
    "#returning an XArray Dataset.\n",
    "\n",
    "#Just need lon, lat, wind, pressure, time for each storm/time entry\n",
    "\n",
    "def readStorms(dict_files, ntime=360):\n",
    "    dsdict_years = dict()\n",
    "    for year in dict_files.keys():\n",
    "        #Loop once through the file to find the number of storms\n",
    "        storm_total = 0\n",
    "        for line in dict_files[year]:\n",
    "            if line[0:5] == 'start':\n",
    "                storm_total = storm_total + 1\n",
    "        \n",
    "        #Initialize arrays (1 line per storm) for each variable for the year\n",
    "        #(Convert time into datetime later)\n",
    "        year_of_lon = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_lat = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_wind = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_pressure = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_year = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_month = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_day = np.ones([storm_total,ntime])*np.nan\n",
    "        year_of_hour = np.ones([storm_total,ntime])*np.nan\n",
    "        \n",
    "        #Loop through the storms and populate the arrays\n",
    "        storm_counter = -1\n",
    "        for line in dict_files[year]:\n",
    "            if line[0:5] == 'start': #New storm\n",
    "                storm_counter = storm_counter + 1\n",
    "                entry_counter = 0\n",
    "                \n",
    "            else: #Populate line for existing storm\n",
    "                split = line.split()\n",
    "                year_of_lon[storm_counter, entry_counter] = split[0]\n",
    "                year_of_lat[storm_counter, entry_counter] = split[1]\n",
    "                year_of_wind[storm_counter, entry_counter] = split[2]\n",
    "                year_of_pressure[storm_counter, entry_counter] = split[3]\n",
    "                year_of_year[storm_counter, entry_counter] = split[4]\n",
    "                year_of_month[storm_counter, entry_counter] = split[5]\n",
    "                year_of_day[storm_counter, entry_counter] = split[6]\n",
    "                year_of_hour[storm_counter, entry_counter] = split[7]\n",
    "                entry_counter = entry_counter + 1\n",
    "    \n",
    "        #Convert the 2D NumPy arrays into XArray DataArrays\n",
    "        da_lon = xr.DataArray(year_of_lon, name='lon',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_lat = xr.DataArray(year_of_lat, name='lat',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_wind = xr.DataArray(year_of_wind, name='wind',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_pressure = xr.DataArray(year_of_pressure, name='pressure', \n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_year = xr.DataArray(year_of_year, name='year',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_month = xr.DataArray(year_of_month, name='month',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_day = xr.DataArray(year_of_day, name='day',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        da_hour = xr.DataArray(year_of_hour, name='hour',\n",
    "                              coords=[pd.Index(np.arange(storm_total), name='storm'), \n",
    "                                    pd.Index(np.arange(ntime), name='date_time')])\n",
    "        \n",
    "        #For the \"time\" one, more tricky--need to create a datetime64 object from the year, month, day, hour\n",
    "        #Or a Pandas datetime?\n",
    "        #(May not actually need this for anything other than subsetting)\n",
    "        #(Model output is already 6-hourly for ACE, etc.)\n",
    "        #Honestly, don't bother with fancy datetime for now--can add it later if it is truly necessary.\n",
    "    \n",
    "        #Merge into Dataset for the year\n",
    "        dsdict_years[year] = xr.merge([da_lon, da_lat, da_wind, da_pressure, da_year, da_month, da_day, da_hour])\n",
    "    \n",
    "    #Concatenate across the years along storm dimension\n",
    "    ds=xr.concat(dsdict_years.values(), dim='storm')\n",
    "    \n",
    "    return(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tracks_v1 = readStorms(dict_files_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tracks_v2 = readStorms(dict_files_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   ADD ADDITIONAL VARIABLES   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind speed in knots: \n",
    "ds_tracks_v1['wind_kts'] = ds_tracks_v1['wind']*1.94384\n",
    "ds_tracks_v2['wind_kts'] = ds_tracks_v2['wind']*1.94384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "90\n",
      "0\n",
      "359\n",
      "180\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "#Testing modulus for converting lon from (-180,180) to (0,360)\n",
    "print(np.mod(-90+360,360))\n",
    "print(np.mod(90+360,360))\n",
    "print(np.mod(0+360,360))\n",
    "print(np.mod(-1+360,360))\n",
    "print(np.mod(180+360,360))\n",
    "print(np.mod(-180+360,360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Longitude from 0 to 360\n",
    "ds_tracks_v1['lon360'] = np.mod(ds_tracks_v1['lon']+360,360)\n",
    "ds_tracks_v2['lon360'] = np.mod(ds_tracks_v2['lon']+360,360)\n",
    "#This is orders of magnitude faster than looping through all the longitudes and assigning values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regional flags\n",
    "\n",
    "#Northern hemisphere\n",
    "ds_tracks_v1['in_NH'] = ds_tracks_v1['lat'] >= 0 \n",
    "\n",
    "#Southern hemisphere\n",
    "ds_tracks_v1['in_SH'] = ds_tracks_v1['lat'] < 0 \n",
    "\n",
    "#Regions as defined by Jeff (based on 0-360 longitude)\n",
    "#10-22-20: changing NI/WP and EP/NA boundaries to be more accurate--\n",
    "#Gulf of Thailand in Pacific not Indian, and don't include swath of NE Pacific in Atlantic, \n",
    "#using piecewise-linear boundaries. \n",
    "\n",
    "#North Indian ocean\n",
    "ds_tracks_v1['in_NI'] = np.logical_and(ds_tracks_v1['lon360']>=35, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v1['lat']>8, \n",
    "                                                                    ds_tracks_v1['lon360']<99),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v1['lat']-4)<=4, \n",
    "                                                                    ds_tracks_v1['lon360']<((-3./4.)*ds_tracks_v1['lat']+105))))\n",
    "#Western Pacific\n",
    "ds_tracks_v1['in_WP'] = np.logical_and(ds_tracks_v1['lon360']<200, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v1['lat']>8, \n",
    "                                                                    ds_tracks_v1['lon360']>=99),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v1['lat']-4)<=4, \n",
    "                                                                    ds_tracks_v1['lon360']>=((-3./4.)*ds_tracks_v1['lat']+105))))\n",
    "#Eastern Pacific\n",
    "ds_tracks_v1['in_EP'] = np.logical_and(ds_tracks_v1['lon360']>=200, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v1['lat']>24,\n",
    "                                                                    ds_tracks_v1['lon360']<253),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v1['lat']-12)<=12, \n",
    "                                                                    ds_tracks_v1['lon360']<((-7./4.)*ds_tracks_v1['lat']+295))))\n",
    "\n",
    "#North Atlantic\n",
    "ds_tracks_v1['in_NA'] = np.logical_or(np.logical_and(ds_tracks_v1['lat']>24,\n",
    "                                                     ds_tracks_v1['lon360']>=253),\n",
    "                                      np.logical_and(np.abs(ds_tracks_v1['lat']-12)<=12, \n",
    "                                                     ds_tracks_v1['lon360']>=((-7./4.)*ds_tracks_v1['lat']+295)))\n",
    "\n",
    "#South Indian\n",
    "ds_tracks_v1['in_SI'] = np.logical_and(np.logical_and(ds_tracks_v1['lon360']>=25, \n",
    "                                                      ds_tracks_v1['lon360']<105), \n",
    "                                       ds_tracks_v1['lat']<0)\n",
    "#Australian region\n",
    "ds_tracks_v1['in_AUS'] = np.logical_and(np.logical_and(ds_tracks_v1['lon360']>=105, \n",
    "                                                      ds_tracks_v1['lon360']<165), \n",
    "                                       ds_tracks_v1['lat']<0)\n",
    "#South Pacific\n",
    "ds_tracks_v1['in_SP'] = np.logical_and(np.logical_and(ds_tracks_v1['lon360']>=165, \n",
    "                                                      ds_tracks_v1['lon360']<290), \n",
    "                                       ds_tracks_v1['lat']<0)\n",
    "#South Atlantic\n",
    "ds_tracks_v1['in_SA'] = np.logical_and(np.logical_and(ds_tracks_v1['lon360']>=290, \n",
    "                                                      ds_tracks_v1['lon360']<360), \n",
    "                                       ds_tracks_v1['lat']<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for v2\n",
    "\n",
    "#Northern hemisphere\n",
    "ds_tracks_v2['in_NH'] = ds_tracks_v2['lat'] >= 0 \n",
    "\n",
    "#Southern hemisphere\n",
    "ds_tracks_v2['in_SH'] = ds_tracks_v2['lat'] < 0 \n",
    "\n",
    "#North Indian ocean\n",
    "ds_tracks_v2['in_NI'] = np.logical_and(ds_tracks_v2['lon360']>=35, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v2['lat']>8, \n",
    "                                                                    ds_tracks_v2['lon360']<99),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v2['lat']-4)<=4, \n",
    "                                                                    ds_tracks_v2['lon360']<((-3./4.)*ds_tracks_v2['lat']+105))))\n",
    "#Western Pacific\n",
    "ds_tracks_v2['in_WP'] = np.logical_and(ds_tracks_v2['lon360']<200, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v2['lat']>8, \n",
    "                                                                    ds_tracks_v2['lon360']>=99),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v2['lat']-4)<=4, \n",
    "                                                                    ds_tracks_v2['lon360']>=((-3./4.)*ds_tracks_v2['lat']+105))))\n",
    "#Eastern Pacific\n",
    "ds_tracks_v2['in_EP'] = np.logical_and(ds_tracks_v2['lon360']>=200, \n",
    "                                       np.logical_or(np.logical_and(ds_tracks_v2['lat']>24,\n",
    "                                                                    ds_tracks_v2['lon360']<253),\n",
    "                                                     np.logical_and(np.abs(ds_tracks_v2['lat']-12)<=12, \n",
    "                                                                    ds_tracks_v2['lon360']<((-7./4.)*ds_tracks_v2['lat']+295))))\n",
    "\n",
    "#North Atlantic\n",
    "ds_tracks_v2['in_NA'] = np.logical_or(np.logical_and(ds_tracks_v2['lat']>24,\n",
    "                                                     ds_tracks_v2['lon360']>=253),\n",
    "                                      np.logical_and(np.abs(ds_tracks_v2['lat']-12)<=12, \n",
    "                                                     ds_tracks_v2['lon360']>=((-7./4.)*ds_tracks_v2['lat']+295)))\n",
    "\n",
    "#South Indian\n",
    "ds_tracks_v2['in_SI'] = np.logical_and(np.logical_and(ds_tracks_v2['lon360']>=25, \n",
    "                                                      ds_tracks_v2['lon360']<105), \n",
    "                                       ds_tracks_v2['lat']<0)\n",
    "#Australian region\n",
    "ds_tracks_v2['in_AUS'] = np.logical_and(np.logical_and(ds_tracks_v2['lon360']>=105, \n",
    "                                                      ds_tracks_v2['lon360']<165), \n",
    "                                       ds_tracks_v2['lat']<0)\n",
    "#South Pacific\n",
    "ds_tracks_v2['in_SP'] = np.logical_and(np.logical_and(ds_tracks_v2['lon360']>=165, \n",
    "                                                      ds_tracks_v2['lon360']<290), \n",
    "                                       ds_tracks_v2['lat']<0)\n",
    "#South Atlantic\n",
    "ds_tracks_v2['in_SA'] = np.logical_and(np.logical_and(ds_tracks_v2['lon360']>=290, \n",
    "                                                      ds_tracks_v2['lon360']<360), \n",
    "                                       ds_tracks_v2['lat']<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genesis lat/lon\n",
    "ds_tracks_v1['gen_lat'] = ds_tracks_v1['lat'].isel(date_time=0)\n",
    "ds_tracks_v2['gen_lat'] = ds_tracks_v2['lat'].isel(date_time=0)\n",
    "\n",
    "ds_tracks_v1['gen_lon'] = ds_tracks_v1['lon'].isel(date_time=0)\n",
    "ds_tracks_v2['gen_lon'] = ds_tracks_v2['lon'].isel(date_time=0)\n",
    "\n",
    "ds_tracks_v1['gen_lon360'] = ds_tracks_v1['lon360'].isel(date_time=0)\n",
    "ds_tracks_v2['gen_lon360'] = ds_tracks_v2['lon360'].isel(date_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genesis region from the 8 regions: might be impossible to avoid a loop given flag definition\n",
    "#Actually just keep a flag in case I add overlapping regions later.\n",
    "#(This will be harder for )\n",
    "for region in ['NH', 'SH', 'NI', 'WP', 'EP', 'NA', 'SI', 'AUS', 'SP', 'SA']:\n",
    "    ds_tracks_v1['gen_'+region] = ds_tracks_v1['in_'+region].isel(date_time=0)\n",
    "    ds_tracks_v2['gen_'+region] = ds_tracks_v2['in_'+region].isel(date_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#southern hemisphere season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tracks_v1['sh_season'] = ds_tracks_v1['year']-0.5+np.floor(ds_tracks_v1['month']/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tracks_v2['sh_season'] = ds_tracks_v2['year']-0.5+np.floor(ds_tracks_v2['month']/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   SAVE NetCDF FILES   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tracks_v1.to_netcdf('nc_from_xarray/zhao_tracks_v1.nc')\n",
    "ds_tracks_v2.to_netcdf('nc_from_xarray/zhao_tracks_v2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   (Random Testing Below)   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:     (date_time: 360, storm: 1157)\n",
      "Coordinates:\n",
      "  * date_time   (date_time) int64 0 1 2 3 4 5 6 ... 353 354 355 356 357 358 359\n",
      "  * storm       (storm) int64 0 1 2 3 4 5 6 7 8 9 ... 46 47 48 49 50 51 52 53 54\n",
      "Data variables:\n",
      "    lon         (storm, date_time) float64 158.8 159.2 160.2 ... nan nan nan\n",
      "    lat         (storm, date_time) float64 -12.25 -12.25 -12.25 ... nan nan nan\n",
      "    wind        (storm, date_time) float64 16.6 18.14 17.89 ... nan nan nan\n",
      "    pressure    (storm, date_time) float64 9.95 9.9 9.92 9.9 ... nan nan nan nan\n",
      "    year        (storm, date_time) float64 1.98e+03 1.98e+03 ... nan nan\n",
      "    month       (storm, date_time) float64 1.0 1.0 1.0 1.0 ... nan nan nan nan\n",
      "    day         (storm, date_time) float64 10.0 10.0 10.0 10.0 ... nan nan nan\n",
      "    hour        (storm, date_time) float64 0.0 6.0 12.0 18.0 ... nan nan nan nan\n",
      "    wind_kts    (storm, date_time) float64 32.27 35.26 34.78 ... nan nan nan\n",
      "    lon360      (storm, date_time) float64 158.8 159.2 160.2 ... nan nan nan\n",
      "    in_NH       (storm, date_time) bool False False False ... False False False\n",
      "    in_SH       (storm, date_time) bool True True True ... False False False\n",
      "    in_NI       (storm, date_time) bool False False False ... False False False\n",
      "    in_WP       (storm, date_time) bool False False False ... False False False\n",
      "    in_EP       (storm, date_time) bool False False False ... False False False\n",
      "    in_NA       (storm, date_time) bool False False False ... False False False\n",
      "    in_SI       (storm, date_time) bool False False False ... False False False\n",
      "    in_AUS      (storm, date_time) bool True True True ... False False False\n",
      "    in_SP       (storm, date_time) bool False False False ... False False False\n",
      "    in_SA       (storm, date_time) bool False False False ... False False False\n",
      "    gen_lat     (storm) float64 -12.25 -17.75 -13.75 ... -9.25 12.25 -13.25\n",
      "    gen_lon     (storm) float64 158.8 72.25 163.2 -168.2 ... 73.25 131.2 65.25\n",
      "    gen_lon360  (storm) float64 158.8 72.25 163.2 191.8 ... 73.25 131.2 65.25\n",
      "    gen_NH      (storm) bool False False False False ... True False True False\n",
      "    gen_SH      (storm) bool True True True True True ... False True False True\n",
      "    gen_NI      (storm) bool False False False False ... False False False False\n",
      "    gen_WP      (storm) bool False False False False ... True False True False\n",
      "    gen_EP      (storm) bool False False False False ... False False False False\n",
      "    gen_NA      (storm) bool False False False False ... False False False False\n",
      "    gen_SI      (storm) bool False True False False ... False True False True\n",
      "    gen_AUS     (storm) bool True False True False ... False False False False\n",
      "    gen_SP      (storm) bool False False False True ... False False False False\n",
      "    gen_SA      (storm) bool False False False False ... False False False False\n",
      "    sh_season   (storm, date_time) float64 1.98e+03 1.98e+03 ... nan nan\n"
     ]
    }
   ],
   "source": [
    "print(ds_tracks_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = readStorms(dict_files_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (date_time: 360, storm: 987)\n",
      "Coordinates:\n",
      "  * date_time  (date_time) int64 0 1 2 3 4 5 6 7 ... 353 354 355 356 357 358 359\n",
      "  * storm      (storm) int64 0 1 2 3 4 5 6 7 8 9 ... 34 35 36 37 38 39 40 41 42\n",
      "Data variables:\n",
      "    lon        (storm, date_time) float64 164.2 164.2 164.2 ... nan nan nan\n",
      "    lat        (storm, date_time) float64 -18.25 -18.75 -19.25 ... nan nan nan\n",
      "    wind       (storm, date_time) float64 15.8 17.27 16.72 17.28 ... nan nan nan\n",
      "    pressure   (storm, date_time) float64 9.85 9.84 9.82 9.84 ... nan nan nan\n",
      "    year       (storm, date_time) float64 1.98e+03 1.98e+03 1.98e+03 ... nan nan\n",
      "    month      (storm, date_time) float64 1.0 1.0 1.0 1.0 ... nan nan nan nan\n",
      "    day        (storm, date_time) float64 1.0 1.0 1.0 2.0 ... nan nan nan nan\n",
      "    hour       (storm, date_time) float64 6.0 12.0 18.0 0.0 ... nan nan nan nan\n",
      "<xarray.DataArray 'year' (storm: 987, date_time: 360)>\n",
      "array([[1980., 1980., 1980., ...,   nan,   nan,   nan],\n",
      "       [1980., 1980., 1980., ...,   nan,   nan,   nan],\n",
      "       [1980., 1980., 1980., ...,   nan,   nan,   nan],\n",
      "       ...,\n",
      "       [2000., 2000., 2000., ...,   nan,   nan,   nan],\n",
      "       [2000., 2000., 2000., ...,   nan,   nan,   nan],\n",
      "       [2000., 2000., 2000., ...,   nan,   nan,   nan]])\n",
      "Coordinates:\n",
      "  * date_time  (date_time) int64 0 1 2 3 4 5 6 7 ... 353 354 355 356 357 358 359\n",
      "  * storm      (storm) int64 0 1 2 3 4 5 6 7 8 9 ... 34 35 36 37 38 39 40 41 42\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'start here'[0:5] == 'start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', '15', '1980', '1', '1', '6']\n",
      "['164.25', '-18.25', '15.80', '9.85', '1980', '1', '1', '6']\n"
     ]
    }
   ],
   "source": [
    "#Test string splitting\n",
    "print('start    15  1980     1     1     6\\n'.split())\n",
    "print('  164.25  -18.25   15.80    9.85  1980     1     1     6\\n'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(10)*np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([5,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1980-01-01 00:00:00')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('19800101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2000-01-01')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.datetime64('2000-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the SH season variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(5/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(6/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(7/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(11/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1992-0.5+np.floor(3/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1991-0.5+np.floor(9/6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, this should work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
