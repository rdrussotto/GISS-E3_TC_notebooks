{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook to filter and preprocess the storms in the IBTrACS dataset.\n",
    "#Successor to Explore_IBTrACS_v2.ipynb.\n",
    "\n",
    "#10-19-20: adding regional tags\n",
    "#10-22-20: adding Southern Hemisphere tags; genesis lat/lon; adjusting region boundaries\n",
    "#10-29-20: adding genesis year and month variables\n",
    "#11-05-20: adding variable for the Southern Hemisphere season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What filtering to do? \n",
    "\n",
    "#Drop non-USA variables\n",
    "#Make sure storm reaches at least TS intensity, otherwise delete the whole line\n",
    "#Make sure times are every 6 hours so that ACE, density, etc. can be calculated correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems like this intricate calculation cannot be vectorized and can only be done in a loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How did Jeff do this? \n",
    "#In script: JS_C180v2_ParamSensTest_ObsDiffMaps.m\n",
    "#Had a file called '/discover/nobackup/jdstron1/Observations_MonClimo.mat'\n",
    "#So must have saved that from somewhere else.\n",
    "#Should be in the file Observations_Compilation.m on sc9. \n",
    "#Need to download the sc9 files.\n",
    "\n",
    "#Downloaded this one of Jeff's scripts. \n",
    "#Looks like used the lon_wmo, lat_wmo, time_wmo variables from v03r10 \n",
    "#(which had a slightly different filename)\n",
    "#I hav v04r00.\n",
    "#He did NOT attempt to filter out times that weren't 6-hourly, \n",
    "#or storms that didn't reach 34 knots.\n",
    "\n",
    "#Unless he got the data already filtered by someone else. \n",
    "#Can't find these wmo variables in the dataset I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_IB = xr.open_dataset('IBTrACS.ALL.v04r00.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop non-US variables, and other irrelevant variables with datetime coordinates\n",
    "ds_IB_slim = ds_IB.drop(labels=[\n",
    "    'tokyo_lat'          ,\n",
    "    'tokyo_lon'          ,\n",
    "    'tokyo_grade'        ,\n",
    "    'tokyo_wind'         ,\n",
    "    'tokyo_pres'         ,\n",
    "    'tokyo_r50_dir'      ,\n",
    "    'tokyo_r50_long'     ,\n",
    "    'tokyo_r50_short'    ,\n",
    "    'tokyo_r30_dir'      ,\n",
    "    'tokyo_r30_long'     ,\n",
    "    'tokyo_r30_short'    ,\n",
    "    'tokyo_land'         ,\n",
    "    'cma_lat'            ,\n",
    "    'cma_lon'            ,\n",
    "    'cma_cat'            ,\n",
    "    'cma_wind'           ,\n",
    "    'cma_pres'           ,\n",
    "    'hko_lat'            ,\n",
    "    'hko_lon'            ,\n",
    "    'hko_cat'            ,\n",
    "    'hko_wind'           ,\n",
    "    'hko_pres'           ,\n",
    "    'newdelhi_lat'       ,\n",
    "    'newdelhi_lon'       ,\n",
    "    'newdelhi_grade'     ,\n",
    "    'newdelhi_wind'      ,\n",
    "    'newdelhi_pres'      ,\n",
    "    'newdelhi_ci'        ,\n",
    "    'newdelhi_dp'        ,\n",
    "    'newdelhi_poci'      ,\n",
    "    'reunion_lat'        ,\n",
    "    'reunion_lon'        ,\n",
    "    'reunion_type'       ,\n",
    "    'reunion_wind'       ,\n",
    "    'reunion_pres'       ,\n",
    "    'reunion_tnum'       ,\n",
    "    'reunion_ci'         ,\n",
    "    'reunion_rmw'        ,\n",
    "    'reunion_r34'        ,\n",
    "    'reunion_r50'        ,\n",
    "    'reunion_r64'        ,\n",
    "    'bom_lat'            ,\n",
    "    'bom_lon'            ,\n",
    "    'bom_type'           ,\n",
    "    'bom_wind'           ,\n",
    "    'bom_pres'           ,\n",
    "    'bom_tnum'           ,\n",
    "    'bom_ci'             ,\n",
    "    'bom_rmw'            ,\n",
    "    'bom_r34'            ,\n",
    "    'bom_r50'            ,\n",
    "    'bom_r64'            ,\n",
    "    'bom_roci'           ,\n",
    "    'bom_poci'           ,\n",
    "    'bom_eye'            ,\n",
    "    'bom_pos_method'     ,\n",
    "    'bom_pres_method'    ,\n",
    "    'nadi_lat'           ,\n",
    "    'nadi_lon'           ,\n",
    "    'nadi_cat'           ,\n",
    "    'nadi_wind'          ,\n",
    "    'nadi_pres'          ,\n",
    "    'wellington_lat'     ,\n",
    "    'wellington_lon'     ,\n",
    "    'wellington_wind'    ,\n",
    "    'wellington_pres'    ,\n",
    "    'ds824_lat'          ,\n",
    "    'ds824_lon'          ,\n",
    "    'ds824_stage'        ,\n",
    "    'ds824_wind'         ,\n",
    "    'ds824_pres'         ,\n",
    "    'td9636_lat'         ,\n",
    "    'td9636_lon'         ,\n",
    "    'td9636_stage'       ,\n",
    "    'td9636_wind'        ,\n",
    "    'td9636_pres'        ,\n",
    "    'td9635_lat'         ,\n",
    "    'td9635_lon'         ,\n",
    "    'td9635_wind'        ,\n",
    "    'td9635_pres'        ,\n",
    "    'td9635_roci'        ,\n",
    "    'neumann_lat'        ,\n",
    "    'neumann_lon'        ,\n",
    "    'neumann_class'      ,\n",
    "    'neumann_wind'       ,\n",
    "    'neumann_pres'       ,\n",
    "    'mlc_lat'            ,\n",
    "    'mlc_lon'            ,\n",
    "    'mlc_class'          ,\n",
    "    'mlc_wind'           ,\n",
    "    'mlc_pres'           ,\n",
    "    'bom_gust'           ,\n",
    "    'bom_gust_per'       ,\n",
    "    'reunion_gust'       ,\n",
    "    'reunion_gust_per'   ,\n",
    "    'dist2land'                   ,\n",
    "    'landfall'                   ,\n",
    "    'usa_r34'                   ,\n",
    "    'usa_r50'                   ,\n",
    "    'usa_r64'                   ,\n",
    "    'usa_sshs'                   ,\n",
    "    'usa_poci'                   ,\n",
    "    'usa_roci'                   ,\n",
    "    'usa_rmw'                   ,\n",
    "    'usa_eye'                   ,\n",
    "    'usa_gust'                   ,\n",
    "    'usa_seahgt'                   ,\n",
    "    'usa_searad'                   ,\n",
    "    'storm_speed'                   ,\n",
    "    'storm_dir'                   ,\n",
    "    'nature'                   ,\n",
    "    #'wmo_wind'                   ,\n",
    "    #'wmo_agency'                   ,\n",
    "    #'wmo_pres'                   ,\n",
    "    'track_type'                   ,\n",
    "    'main_track_sid'                   ,\n",
    "    'iflag'                   ,\n",
    "    'basin'                   ,\n",
    "    'subbasin'                   ,\n",
    "    'iso_time'                   ,\n",
    "    'usa_atcf_id'                   ,\n",
    "    'usa_record'                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset for 1980-2000\n",
    "ds_since80 = ds_IB_slim.loc[dict(date_time=ds_IB_slim.date_time, storm=ds_IB_slim.storm[(ds_IB_slim['time.year'] >= 1980)[:,0]])]\n",
    "ds_80_00 = ds_since80.loc[dict(date_time=ds_since80.date_time, storm=ds_since80.storm[(ds_since80['time.year'] < 2001)[:,0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00['usa_wind'])\n",
    "#print(ds_80_00['usa_record'].description) #This variable isn't necessary, things like landfall time\n",
    "#for i in np.arange(2000):\n",
    "    #print(np.mod(ds_80_00['time.hour'].data[i,:], 6))\n",
    "    #Looks like almost all the storms were 3-hourly, but some also have weird hours in the middle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in np.arange(100):\n",
    "#    print(ds_80_00['iso_time'].data[i,:])\n",
    "#Still 3-hourly, just a different datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00.isel(storm=0)['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00['usa_wind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing storm: 0\n",
      "Processing storm: 50\n",
      "Processing storm: 100\n",
      "Processing storm: 150\n",
      "Processing storm: 200\n",
      "Processing storm: 250\n",
      "Processing storm: 300\n",
      "Processing storm: 350\n",
      "Processing storm: 400\n",
      "Processing storm: 450\n",
      "Processing storm: 500\n",
      "Processing storm: 550\n",
      "Processing storm: 600\n",
      "Processing storm: 650\n",
      "Processing storm: 700\n",
      "Processing storm: 750\n",
      "Processing storm: 800\n",
      "Processing storm: 850\n",
      "Processing storm: 900\n",
      "Processing storm: 950\n",
      "Processing storm: 1000\n",
      "Processing storm: 1050\n",
      "Processing storm: 1100\n",
      "Processing storm: 1150\n",
      "Processing storm: 1200\n",
      "Processing storm: 1250\n",
      "Processing storm: 1300\n",
      "Processing storm: 1350\n",
      "Processing storm: 1400\n",
      "Processing storm: 1450\n",
      "Processing storm: 1500\n",
      "Processing storm: 1550\n",
      "Processing storm: 1600\n",
      "Processing storm: 1650\n",
      "Processing storm: 1700\n",
      "Processing storm: 1750\n",
      "Processing storm: 1800\n",
      "Processing storm: 1850\n",
      "Processing storm: 1900\n",
      "Processing storm: 1950\n",
      "Processing storm: 2000\n",
      "Processing storm: 2050\n",
      "Processing storm: 2100\n",
      "Processing storm: 2150\n",
      "Processing storm: 2200\n",
      "Processing storm: 2250\n",
      "Processing storm: 2300\n",
      "Processing storm: 2350\n"
     ]
    }
   ],
   "source": [
    "#Loop through the storms, drop any times not at 0, 6, 12, or 18z, and pad the end with NaNs to keep length the same; \n",
    "#also calculate max lifetime wind and set the entire row to nans if it's less than 34 knots.\n",
    "#Then loop through again and delete any rows that are all nans? (No, taken care of by .where())\n",
    "\n",
    "numStorms = len(ds_80_00.storm)\n",
    "numDT = len(ds_80_00.date_time)\n",
    "#print(numStorms) #2398\n",
    "\n",
    "\n",
    "ds_80_00_processed = ds_80_00.copy()\n",
    "for i in np.arange(numStorms):\n",
    "#for i in np.arange(30):   \n",
    "    if np.mod(i, 50) == 0:\n",
    "        print('Processing storm: '+ str(i))\n",
    "    storm_row = ds_80_00.isel(storm=i)\n",
    "    \n",
    "    usa_lat_temp = storm_row['usa_lat']\n",
    "    usa_lon_temp = storm_row['usa_lon']\n",
    "    usa_wind_temp = storm_row['usa_wind']\n",
    "    usa_pres_temp = storm_row['usa_pres']\n",
    "    usa_status_temp = storm_row['usa_status']\n",
    "    usa_agency_temp = storm_row['usa_agency']\n",
    "    wmo_wind_temp = storm_row['wmo_wind']\n",
    "    wmo_pres_temp = storm_row['wmo_pres']\n",
    "    wmo_agency_temp = storm_row['wmo_agency']\n",
    "    time_temp = storm_row['time']\n",
    "    lat_temp = storm_row['lat']\n",
    "    lon_temp = storm_row['lon']\n",
    "    \n",
    "    #Drop any non-6-hourly time points from each variable\n",
    "    usa_lat_6h = usa_lat_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    usa_lon_6h = usa_lon_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    usa_wind_6h = usa_wind_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    usa_pres_6h = usa_pres_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    usa_status_6h = usa_status_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    usa_agency_6h = usa_agency_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    wmo_wind_6h = wmo_wind_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    wmo_pres_6h = wmo_pres_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    wmo_agency_6h = wmo_agency_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    time_6h = time_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    lat_6h = lat_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    lon_6h = lon_temp.where(np.mod(usa_lat_temp['time.hour'], 6) == 0, drop=True)\n",
    "    \n",
    "    #Pad the row with nans\n",
    "    #This isn't working...\n",
    "    #Running into issue because of the \"coordinates\" not actually being the same as the dimensions\n",
    "    #Instead, assign values in the \"complete\" array to the 6-hourly values and the rest as nans\n",
    "#     len_pad = len(storm_row.date_time) - len(usa_lat_6h)\n",
    "#     index_pad = pd.Index(np.arange(len_pad)+len(storm_row.date_time), name='date_time')\n",
    "#     nan_pad = xr.DataArray(np.ones(len_pad)*np.nan, coords=None, dims=[index_pad])\n",
    "    \n",
    "#     print(usa_lat_6h)\n",
    "#     print(nan_pad)\n",
    "    \n",
    "#     usa_lat_6h_pad = xr.concat([usa_lat_6h, nan_pad], dim='date_time')\n",
    "#     usa_lon_6h_pad = xr.concat([usa_lon_6h, nan_pad], dim='date_time')\n",
    "#     usa_wind_6h_pad = xr.concat([usa_wind_6h, nan_pad], dim='date_time')\n",
    "#     usa_pres_6h_pad = xr.concat([usa_pres_6h, nan_pad], dim='date_time')\n",
    "#     usa_status_6h_pad = xr.concat([usa_status_6h, nan_pad], dim='date_time')\n",
    "#     usa_agency_6h_pad = xr.concat([usa_agency_6h, nan_pad], dim='date_time')\n",
    "    \n",
    "    #Assign the filtered storms to the copy (this is made easier by storm and date_time indices being integers)\n",
    "    len_pad = len(storm_row.date_time) - len(usa_lat_6h)\n",
    "    fill_indices = np.arange(len(usa_lat_6h))\n",
    "    pad_indices = np.arange(len_pad)+len(usa_lat_6h)\n",
    "    \n",
    "    #First assign the times: this one seems more difficult than anything else. \n",
    "    #Specifically, filling rest of values with NaT: how to do this?\n",
    "    #Preallocate empty array and fill each variable with 'NaT'\n",
    "    ds_80_00_processed['time'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = time_6h.data\n",
    "    t_pad = np.empty(len_pad, dtype='datetime64[ns]')\n",
    "    for k in np.arange(len_pad):\n",
    "        t_pad[k] = np.datetime64('NaT')\n",
    "    ds_80_00_processed['time'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = t_pad\n",
    "    \n",
    "    #Assign the rest of the time-varying variables\n",
    "    ds_80_00_processed['usa_lat'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_lat_6h.data\n",
    "    ds_80_00_processed['usa_lat'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['usa_lon'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_lon_6h.data\n",
    "    ds_80_00_processed['usa_lon'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['usa_wind'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_wind_6h.data\n",
    "    ds_80_00_processed['usa_wind'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "\n",
    "    ds_80_00_processed['usa_pres'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_pres_6h.data\n",
    "    ds_80_00_processed['usa_pres'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['usa_status'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_status_6h.data\n",
    "    ds_80_00_processed['usa_status'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['usa_agency'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = usa_agency_6h.data\n",
    "    ds_80_00_processed['usa_agency'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['wmo_wind'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = wmo_wind_6h.data\n",
    "    ds_80_00_processed['wmo_wind'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "\n",
    "    ds_80_00_processed['wmo_pres'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = wmo_pres_6h.data\n",
    "    ds_80_00_processed['wmo_pres'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['wmo_agency'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = wmo_agency_6h.data\n",
    "    ds_80_00_processed['wmo_agency'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['lat'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = lat_6h.data\n",
    "    ds_80_00_processed['lat'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "    ds_80_00_processed['lon'].loc[dict(storm=ds_80_00.storm[i], date_time = fill_indices)] = lon_6h.data\n",
    "    ds_80_00_processed['lon'].loc[dict(storm=ds_80_00.storm[i], date_time = pad_indices)] = np.ones(len_pad)*np.nan\n",
    "    \n",
    "##Calculate maximum wind for each storm (as a new variable?)\n",
    "ds_80_00_processed['max_wind'] = ds_80_00_processed['usa_wind'].max(dim='date_time')\n",
    "\n",
    "#Drop storms where max wind < 34 knots, using dataset.where\n",
    "#Some storms apparently don't have ANY values for usa_wind, maybe had no US reporting at all (duplicates?)\n",
    "#Should drop these too. e.g. np.nan > 34 yields False so this will work for those too.\n",
    "ds_80_00_processed_34 = ds_80_00_processed.where(ds_80_00_processed['max_wind'] >= 34, drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This isn't so bad: seems to take about 6 seconds per 100 storms, or 1000 per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   Add Additional Variables   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Longitude from 0 to 360\n",
    "ds_80_00_processed_34['usa_lon360'] = np.mod(ds_80_00_processed_34['usa_lon']+360,360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regional flags\n",
    "\n",
    "\n",
    "#Northern hemisphere\n",
    "ds_80_00_processed_34['in_NH'] = ds_80_00_processed_34['usa_lat'] >= 0 \n",
    "\n",
    "#Southern hemisphere\n",
    "ds_80_00_processed_34['in_SH'] = ds_80_00_processed_34['usa_lat'] < 0 \n",
    "\n",
    "#North Indian ocean\n",
    "ds_80_00_processed_34['in_NI'] = np.logical_and(ds_80_00_processed_34['usa_lon360']>=35, \n",
    "                                       np.logical_or(np.logical_and(ds_80_00_processed_34['usa_lat']>8, \n",
    "                                                                    ds_80_00_processed_34['usa_lon360']<99),\n",
    "                                                     np.logical_and(np.abs(ds_80_00_processed_34['usa_lat']-4)<=4, \n",
    "                                                                    ds_80_00_processed_34['usa_lon360']<((-3./4.)*ds_80_00_processed_34['usa_lat']+105))))\n",
    "#Western Pacific\n",
    "ds_80_00_processed_34['in_WP'] = np.logical_and(ds_80_00_processed_34['usa_lon360']<200, \n",
    "                                       np.logical_or(np.logical_and(ds_80_00_processed_34['usa_lat']>8, \n",
    "                                                                    ds_80_00_processed_34['usa_lon360']>=99),\n",
    "                                                     np.logical_and(np.abs(ds_80_00_processed_34['usa_lat']-4)<=4, \n",
    "                                                                    ds_80_00_processed_34['usa_lon360']>=((-3./4.)*ds_80_00_processed_34['usa_lat']+105))))\n",
    "#Eastern Pacific\n",
    "ds_80_00_processed_34['in_EP'] = np.logical_and(ds_80_00_processed_34['usa_lon360']>=200, \n",
    "                                       np.logical_or(np.logical_and(ds_80_00_processed_34['usa_lat']>24,\n",
    "                                                                    ds_80_00_processed_34['usa_lon360']<253),\n",
    "                                                     np.logical_and(np.abs(ds_80_00_processed_34['usa_lat']-12)<=12, \n",
    "                                                                    ds_80_00_processed_34['usa_lon360']<((-7./4.)*ds_80_00_processed_34['usa_lat']+295))))\n",
    "\n",
    "#North Atlantic\n",
    "ds_80_00_processed_34['in_NA'] = np.logical_or(np.logical_and(ds_80_00_processed_34['usa_lat']>24,\n",
    "                                                     ds_80_00_processed_34['usa_lon360']>=253),\n",
    "                                      np.logical_and(np.abs(ds_80_00_processed_34['usa_lat']-12)<=12, \n",
    "                                                     ds_80_00_processed_34['usa_lon360']>=((-7./4.)*ds_80_00_processed_34['usa_lat']+295)))\n",
    "\n",
    "#South Indian\n",
    "ds_80_00_processed_34['in_SI'] = np.logical_and(np.logical_and(ds_80_00_processed_34['usa_lon360']>=25, \n",
    "                                                      ds_80_00_processed_34['usa_lon360']<105), \n",
    "                                       ds_80_00_processed_34['usa_lat']<0)\n",
    "#Australian region\n",
    "ds_80_00_processed_34['in_AUS'] = np.logical_and(np.logical_and(ds_80_00_processed_34['usa_lon360']>=105, \n",
    "                                                      ds_80_00_processed_34['usa_lon360']<165), \n",
    "                                       ds_80_00_processed_34['usa_lat']<0)\n",
    "#South Pacific\n",
    "ds_80_00_processed_34['in_SP'] = np.logical_and(np.logical_and(ds_80_00_processed_34['usa_lon360']>=165, \n",
    "                                                      ds_80_00_processed_34['usa_lon360']<290), \n",
    "                                       ds_80_00_processed_34['usa_lat']<0)\n",
    "#South Atlantic\n",
    "ds_80_00_processed_34['in_SA'] = np.logical_and(np.logical_and(ds_80_00_processed_34['usa_lon360']>=290, \n",
    "                                                      ds_80_00_processed_34['usa_lon360']<360), \n",
    "                                       ds_80_00_processed_34['usa_lat']<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genesis lat/lon, based on first non-nan latitude (including 360 degree lon)\n",
    "#Genesis region flags, based on that lat/lon\n",
    "#Done in another loop through the storms\n",
    "\n",
    "#Preallocate with zeros or falses\n",
    "ds_80_00_processed_34['gen_lat'] = ds_80_00_processed_34['numobs']*0 #This should have no nans\n",
    "ds_80_00_processed_34['gen_lon'] = ds_80_00_processed_34['numobs']*0\n",
    "ds_80_00_processed_34['gen_lon360'] = ds_80_00_processed_34['numobs']*0\n",
    "for region in ['NH', 'SH', 'NI', 'WP', 'EP', 'NA', 'SI', 'AUS', 'SP', 'SA']:\n",
    "    ds_80_00_processed_34['gen_'+region] = ds_80_00_processed_34['in_NH'].isel(date_time=0)*False\n",
    "ds_80_00_processed_34['gen_month'] = ds_80_00_processed_34['numobs']*0\n",
    "ds_80_00_processed_34['gen_year'] = ds_80_00_processed_34['numobs']*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([True, False, True])*False #False, False, False. Yes, this preallocation method should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00_processed_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding genesis info for storm: 0\n",
      "Adding genesis info for storm: 50\n",
      "Adding genesis info for storm: 100\n",
      "Adding genesis info for storm: 150\n",
      "Adding genesis info for storm: 200\n",
      "Adding genesis info for storm: 250\n",
      "Adding genesis info for storm: 300\n",
      "Adding genesis info for storm: 350\n",
      "Adding genesis info for storm: 400\n",
      "Adding genesis info for storm: 450\n",
      "Adding genesis info for storm: 500\n",
      "Adding genesis info for storm: 550\n",
      "Adding genesis info for storm: 600\n",
      "Adding genesis info for storm: 650\n",
      "Adding genesis info for storm: 700\n",
      "Adding genesis info for storm: 750\n",
      "Adding genesis info for storm: 800\n",
      "Adding genesis info for storm: 850\n",
      "Adding genesis info for storm: 900\n",
      "Adding genesis info for storm: 950\n",
      "Adding genesis info for storm: 1000\n",
      "Adding genesis info for storm: 1050\n",
      "Adding genesis info for storm: 1100\n",
      "Adding genesis info for storm: 1150\n",
      "Adding genesis info for storm: 1200\n",
      "Adding genesis info for storm: 1250\n",
      "Adding genesis info for storm: 1300\n",
      "Adding genesis info for storm: 1350\n",
      "Adding genesis info for storm: 1400\n",
      "Adding genesis info for storm: 1450\n",
      "Adding genesis info for storm: 1500\n",
      "Adding genesis info for storm: 1550\n",
      "Adding genesis info for storm: 1600\n",
      "Adding genesis info for storm: 1650\n",
      "Adding genesis info for storm: 1700\n",
      "Adding genesis info for storm: 1750\n",
      "Adding genesis info for storm: 1800\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(len(ds_80_00_processed_34.storm)):\n",
    "    if np.mod(i,50) == 0:\n",
    "        print('Adding genesis info for storm: ' + str(i))\n",
    "    storm_lat_raw = ds_80_00_processed_34['usa_lat'].isel(storm=i).data\n",
    "    storm_lon_raw = ds_80_00_processed_34['usa_lon'].isel(storm=i).data\n",
    "    storm_lon360_raw = ds_80_00_processed_34['usa_lon360'].isel(storm=i).data\n",
    "    storm_month_raw = ds_80_00_processed_34['time.month'].isel(storm=i).data\n",
    "    storm_year_raw = ds_80_00_processed_34['time.year'].isel(storm=i).data\n",
    "    \n",
    "    lat_nonan = storm_lat_raw[~np.isnan(storm_lat_raw)]\n",
    "    lon_nonan = storm_lon_raw[~np.isnan(storm_lon_raw)]\n",
    "    lon360_nonan = storm_lon360_raw[~np.isnan(storm_lon360_raw)]\n",
    "    month_nonan = storm_month_raw[~np.isnan(storm_lat_raw)]\n",
    "    year_nonan = storm_year_raw[~np.isnan(storm_lat_raw)]\n",
    "    \n",
    "    gen_lat_i = lat_nonan[0]\n",
    "    gen_lon_i = lon_nonan[0]\n",
    "    gen_lon360_i = lon360_nonan[0]\n",
    "    gen_month_i = month_nonan[0]\n",
    "    gen_year_i = year_nonan[0]\n",
    "    \n",
    "    ds_80_00_processed_34['gen_lat'][i] = gen_lat_i\n",
    "    ds_80_00_processed_34['gen_lon'][i] = gen_lon_i\n",
    "    ds_80_00_processed_34['gen_lon360'][i] = gen_lon360_i\n",
    "    ds_80_00_processed_34['gen_month'][i] = gen_month_i\n",
    "    ds_80_00_processed_34['gen_year'][i] = gen_year_i\n",
    "    \n",
    "    \n",
    "    #Just do each region check again--easier than getting the date_time index of the first non-nan\n",
    "\n",
    "    ds_80_00_processed_34['gen_NH'][i] = gen_lat_i >= 0 \n",
    "\n",
    "    ds_80_00_processed_34['gen_SH'][i] = gen_lat_i < 0 \n",
    "\n",
    "    ds_80_00_processed_34['gen_NI'][i] = np.logical_and(gen_lon360_i>=35, \n",
    "                                           np.logical_or(np.logical_and(gen_lat_i>8, \n",
    "                                                                        gen_lon360_i<99),\n",
    "                                                         np.logical_and(np.abs(gen_lat_i-4)<=4, \n",
    "                                                                        gen_lon360_i<((-3./4.)*gen_lat_i+105))))\n",
    "\n",
    "    ds_80_00_processed_34['gen_WP'][i] = np.logical_and(gen_lon360_i<200, \n",
    "                                           np.logical_or(np.logical_and(gen_lat_i>8, \n",
    "                                                                        gen_lon360_i>=99),\n",
    "                                                         np.logical_and(np.abs(gen_lat_i-4)<=4, \n",
    "                                                                        gen_lon360_i>=((-3./4.)*gen_lat_i+105))))\n",
    "\n",
    "    ds_80_00_processed_34['gen_EP'][i] = np.logical_and(gen_lon360_i>=200, \n",
    "                                           np.logical_or(np.logical_and(gen_lat_i>24,\n",
    "                                                                        gen_lon360_i<253),\n",
    "                                                         np.logical_and(np.abs(gen_lat_i-12)<=12, \n",
    "                                                                        gen_lon360_i<((-7./4.)*gen_lat_i+295))))\n",
    "\n",
    "    ds_80_00_processed_34['gen_NA'][i] = np.logical_or(np.logical_and(gen_lat_i>24,\n",
    "                                                         gen_lon360_i>=253),\n",
    "                                          np.logical_and(np.abs(gen_lat_i-12)<=12, \n",
    "                                                         gen_lon360_i>=((-7./4.)*gen_lat_i+295)))\n",
    "\n",
    "    ds_80_00_processed_34['gen_SI'][i] = np.logical_and(np.logical_and(gen_lon360_i>=25, \n",
    "                                                          gen_lon360_i<105), \n",
    "                                           gen_lat_i<0)\n",
    "\n",
    "    ds_80_00_processed_34['gen_AUS'][i] = np.logical_and(np.logical_and(gen_lon360_i>=105, \n",
    "                                                          gen_lon360_i<165), \n",
    "                                           gen_lat_i<0)\n",
    "\n",
    "    ds_80_00_processed_34['gen_SP'][i] = np.logical_and(np.logical_and(gen_lon360_i>=165, \n",
    "                                                          gen_lon360_i<290), \n",
    "                                           gen_lat_i<0)\n",
    "\n",
    "    ds_80_00_processed_34['gen_SA'][i] = np.logical_and(np.logical_and(gen_lon360_i>=290, \n",
    "                                                          gen_lon360_i<360), \n",
    "                                           gen_lat_i<0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Southern Hemisphere season based on month and year\n",
    "#Really only need for the genesis time for the statistics I'm doing. More complex to do it\n",
    "#for every data point.\n",
    "ds_80_00_processed_34['gen_sh_season']=(ds_80_00_processed_34['gen_year']\n",
    "                                        - 0.5 + np.floor(ds_80_00_processed_34['gen_month']/6.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:        (date_time: 360, storm: 1817)\n",
      "Coordinates:\n",
      "    time           (storm, date_time) datetime64[ns] 1980-01-01T00:00:00.000040448 ... NaT\n",
      "    lat            (storm, date_time) float32 -12.5 -11.914368 -11.5 ... nan nan\n",
      "    lon            (storm, date_time) float32 172.5 172.41243 172.5 ... nan nan\n",
      "Dimensions without coordinates: date_time, storm\n",
      "Data variables:\n",
      "    numobs         (storm) float32 41.0 79.0 69.0 129.0 ... 48.0 61.0 103.0\n",
      "    sid            (storm) object b'1980001S13173' ... b'2000366S09068'\n",
      "    season         (storm) float32 1980.0 1980.0 1980.0 ... 2000.0 2000.0 2001.0\n",
      "    number         (storm) float64 1.0 3.0 5.0 9.0 ... 109.0 112.0 113.0 114.0\n",
      "    name           (storm) object b'PENI' b'PAUL' b'AMY' ... b'SOULIK' b'ANDO'\n",
      "    source_usa     (storm) object b'bsh051980.txt' ... b'bsh042001.txt'\n",
      "    source_jma     (storm) object b'' b'' ... b''\n",
      "    source_cma     (storm) object b'' b'' ... b''\n",
      "    source_hko     (storm) object b'' b'' ... b''\n",
      "    source_new     (storm) object b'' b'' b'' ... b'' b''\n",
      "    source_reu     (storm) object b'' ... b'WMO-Best-Tracks.txt:Line=19257:ando'\n",
      "    source_bom     (storm) object b'' b'IDCKMSTM0S.csv:Line=11978:PAUL' ... b''\n",
      "    source_nad     (storm) object b'' b'' b'' b'' b'' ... b'' b'' b'' b'' b''\n",
      "    source_wel     (storm) object b'TC_BT_1967_2006.csv:Line=1342:PENI' ... b''\n",
      "    source_td5     (storm) object b'' b'' b'' b'' b'' ... b'' b'' b'' b'' b''\n",
      "    source_td6     (storm) object b'cons_worldwide_trop_cyclone_18710101-19891231-006:Line=10935' ... b''\n",
      "    source_ds8     (storm) object b's_hem.dat:1410:05P' ... b''\n",
      "    source_neu     (storm) object b'tracks.sh:Storm=568:PENI:BSH0580:JTWC' ... b'tracks.sh:Storm=1207:ANDO:BSH042001:JTWC'\n",
      "    source_mlc     (storm) object b'' b'' b'' b'' b'' ... b'' b'' b'' b'' b''\n",
      "    wmo_wind       (storm, date_time) float32 nan nan nan nan ... nan nan nan\n",
      "    wmo_pres       (storm, date_time) float32 nan nan nan nan ... nan nan nan\n",
      "    wmo_agency     (storm, date_time) object b'' b'' b'' ... b'nan' b'nan'\n",
      "    usa_agency     (storm, date_time) object b'jtwc_sh' b'jtwc_sh' ... b'nan'\n",
      "    usa_lat        (storm, date_time) float32 -12.5 -11.9 -11.5 ... nan nan nan\n",
      "    usa_lon        (storm, date_time) float32 172.5 172.4 172.5 ... nan nan nan\n",
      "    usa_status     (storm, date_time) object b'' b'' b'' ... b'na' b'na' b'na'\n",
      "    usa_wind       (storm, date_time) float32 25.0 25.0 25.0 ... nan nan nan\n",
      "    usa_pres       (storm, date_time) float32 nan nan nan nan ... nan nan nan\n",
      "    max_wind       (storm) float32 65.0 60.0 100.0 70.0 ... 65.0 110.0 120.0\n",
      "    usa_lon360     (storm, date_time) float32 172.5 172.40002 172.5 ... nan nan\n",
      "    in_NH          (storm, date_time) bool False False False ... False False\n",
      "    in_SH          (storm, date_time) bool True True True ... False False False\n",
      "    in_NI          (storm, date_time) bool False False False ... False False\n",
      "    in_WP          (storm, date_time) bool False False False ... False False\n",
      "    in_EP          (storm, date_time) bool False False False ... False False\n",
      "    in_NA          (storm, date_time) bool False False False ... False False\n",
      "    in_SI          (storm, date_time) bool False False False ... False False\n",
      "    in_AUS         (storm, date_time) bool False False False ... False False\n",
      "    in_SP          (storm, date_time) bool True True True ... False False False\n",
      "    in_SA          (storm, date_time) bool False False False ... False False\n",
      "    gen_lat        (storm) float32 -12.5 -14.0 -13.2 -19.3 ... 7.3 7.0 -8.7\n",
      "    gen_lon        (storm) float32 172.5 136.0 120.0 57.1 ... 85.4 130.2 67.4\n",
      "    gen_lon360     (storm) float32 172.5 136.0 120.0 ... 130.20001 67.399994\n",
      "    gen_NH         (storm) bool False False False False ... True True False\n",
      "    gen_SH         (storm) bool True True True True ... True False False True\n",
      "    gen_NI         (storm) bool False False False False ... True False False\n",
      "    gen_WP         (storm) bool False False False False ... False True False\n",
      "    gen_EP         (storm) bool False False False False ... False False False\n",
      "    gen_NA         (storm) bool False False False False ... False False False\n",
      "    gen_SI         (storm) bool False False False True ... False False True\n",
      "    gen_AUS        (storm) bool False True True False ... True False False False\n",
      "    gen_SP         (storm) bool True False False False ... False False False\n",
      "    gen_SA         (storm) bool False False False False ... False False False\n",
      "    gen_month      (storm) float32 1.0 1.0 1.0 1.0 1.0 ... 12.0 12.0 12.0 12.0\n",
      "    gen_year       (storm) float32 1980.0 1980.0 1980.0 ... 2000.0 2000.0 2000.0\n",
      "    gen_sh_season  (storm) float32 1979.5 1979.5 1979.5 ... 2000.5 2000.5 2000.5\n",
      "Attributes:\n",
      "    title:                      IBTrACS - International Best Track Archive fo...\n",
      "    summary:                    The intent of the IBTrACS project is to overc...\n",
      "    source:                     The original data are tropical cyclone positi...\n",
      "    Conventions:                ACDD-1.3\n",
      "    Conventions_note:           Data are nearly CF-1.7 compliant. The sole is...\n",
      "    product_version:            v04r00\n",
      "    project:                    International Best Track Archive for Climate ...\n",
      "    processing_level:           NOAA Processing Level 2, Data products are de...\n",
      "    acknowledgement:            IBTrACS was produced by a team of scientists ...\n",
      "    references:                 https://www.ncdc.noaa.gov/ibtracs/, doi:10.11...\n",
      "    institution:                National Centers for Environmental Informatio...\n",
      "    publisher_type:             institution\n",
      "    publisher_name:             National Centers for Environmental Informatio...\n",
      "    publisher_email:            ncei.sat.info@noaa.gov\n",
      "    publisher_institution:      National Centers for Environmental Informatio...\n",
      "    publisher_url:              https://ncei.noaa.gov/\n",
      "    creator_type:               group\n",
      "    creator_name:               IBTrACS Science Team (Ken Knapp, Howard Diamo...\n",
      "    creator_institution:        National Centers for Environmental Informatio...\n",
      "    creator_email:              ibtracs.team@noaa.gov, ken.knapp@noaa.gov\n",
      "    creator_url:                https://www.ncdc.noaa.gov/ibtracs/\n",
      "    contributor_name:           \"National Hurricane Center, National Weather ...\n",
      "    contributor_role:           These agencies and people provide track data ...\n",
      "    date_created:               2020-08-08 03:51:21\n",
      "    date_issued:                2020-08-08 03:51:21\n",
      "    time_coverage_start:        1842-10-25T03:00:00\n",
      "    time_coverage_end:          2020-08-08 03:51:21\n",
      "    geospatial_lat_min:         80.0\n",
      "    geospatial_lat_max:         -80.0\n",
      "    geospatial_lat_units:       degrees_north\n",
      "    geospatial_lat_resolution:  0.10\n",
      "    geospatial_lon_min:         -180.0\n",
      "    geospatial_lon_max:          180.0\n",
      "    geospatial_lon_units:       degrees_east\n",
      "    geospatial_lon_resolution:  0.10\n",
      "    geospatial_vertical_min:    not applicable\n",
      "    geospatial_vertical_max:    not applicable \n",
      "    naming_authority:           gov.noaa.ncei\n",
      "    id:                         1842298N11080.ibtracs_int.v04r00.nc\n",
      "    metadata_link:              doi:10.25921/82ty-9e16\n",
      "    keywords:                   EARTH SCIENCE> ATMOSPHERE> WEATHER EVENTS> TR...\n",
      "    keywords_vocabulary:        GCMD Science Keywords Version 8.6\n",
      "    standard_name_vocabulary:   CF Standard Name Table v52\n",
      "    history:                    Sun Sep 20 04:38:01 2020: ncks -a -4 -L 5 tem...\n",
      "    license:                    These data may be redistributed and used with...\n",
      "    featureType:                trajectory\n",
      "    cdm_data_type:              Trajectory\n",
      "    comment:                    The tracks of TCs generally look like a traje...\n",
      "    nco_openmp_thread_number:   1\n",
      "    NCO:                        4.4.3\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   Save as NetCDF   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_80_00_processed_34.to_netcdf('data_IBTrACS/IBTrACS_1980_2000_6h_min34_saved_20201014.nc')\n",
    "#ds_80_00_processed_34.to_netcdf('data_IBTrACS/IBTrACS_1980_2000_6h_min34_saved_20201019.nc') #With regional tags\n",
    "#ds_80_00_processed_34.to_netcdf('data_IBTrACS/IBTrACS_1980_2000_6h_min34_saved_20201022.nc') #With southern hemisphere and genesis regions\n",
    "#ds_80_00_processed_34.to_netcdf('data_IBTrACS/IBTrACS_1980_2000_6h_min34_saved_20201029.nc') #With genesis month and year #Accidentally overwrote with the next one\n",
    "ds_80_00_processed_34.to_netcdf('data_IBTrACS/IBTrACS_1980_2000_6h_min34_saved_20201105.nc') #With genesis SH season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   Explore output   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_80_00_processed_34)\n",
    "#Wind up with 1817 after excluding TDs and storms without US winds reported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 25. 25. ... nan nan nan]\n",
      " [nan 25. 25. ... nan nan nan]\n",
      " [40. 40. 40. ... nan nan nan]\n",
      " ...\n",
      " [30. 30. 30. ... nan nan nan]\n",
      " [20. 25. 25. ... nan nan nan]\n",
      " [30. 20. 20. ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34['usa_wind'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34['usa_pres'].data)\n",
    "#Huh? Don't have this for anything?\n",
    "#Maybe should've kept wmo_pres, wmo_wind, wmo_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [25. 25. 30. ... nan nan nan]\n",
      " [20. 25. 30. ... nan nan nan]\n",
      " ...\n",
      " [25. 25. 25. ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [25. 25. 25. ... nan nan nan]]\n",
      "<xarray.DataArray 'wmo_wind' (storm: 1817, date_time: 360)>\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [25., 25., 30., ..., nan, nan, nan],\n",
      "       [20., 25., 30., ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [25., 25., 25., ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [25., 25., 25., ..., nan, nan, nan]], dtype=float32)\n",
      "Coordinates:\n",
      "    time     (storm, date_time) datetime64[ns] 1980-01-01T00:00:00.000040448 ... NaT\n",
      "    lat      (storm, date_time) float32 -12.5 -11.914368 -11.5 ... nan nan nan\n",
      "    lon      (storm, date_time) float32 172.5 172.41243 172.5 ... nan nan nan\n",
      "Dimensions without coordinates: storm, date_time\n",
      "Attributes:\n",
      "    long_name:              Maximum sustained wind speed from Official WMO ag...\n",
      "    units:                  kts\n",
      "    coverage_content_type:  physicalMeasurement\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34['wmo_wind'].data)\n",
    "print(ds_80_00_processed_34['wmo_wind'])\n",
    "#Some nans for WMO where USA has stuff; also nans for pressure in these cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan 29. ... nan nan nan]\n",
      " [25. 25. 30. ... nan nan nan]\n",
      " ...\n",
      " [25. 25. 25. ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [25. 25. 25. ... nan nan nan]]\n",
      "<xarray.DataArray 'wmo_wind' (storm: 2398, date_time: 360)>\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, 29., ..., nan, nan, nan],\n",
      "       [25., 25., 30., ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [25., 25., 25., ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [25., 25., 25., ..., nan, nan, nan]], dtype=float32)\n",
      "Coordinates:\n",
      "    time     (storm, date_time) datetime64[ns] 1980-01-01T00:00:00.000040448 ... NaT\n",
      "    lat      (storm, date_time) float32 ...\n",
      "    lon      (storm, date_time) float32 ...\n",
      "Dimensions without coordinates: storm, date_time\n",
      "Attributes:\n",
      "    long_name:              Maximum sustained wind speed from Official WMO ag...\n",
      "    units:                  kts\n",
      "    coverage_content_type:  physicalMeasurement\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00['wmo_wind'].data)\n",
    "print(ds_80_00['wmo_wind'])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan   nan   nan ...   nan   nan   nan]\n",
      " [1000. 1001.  999. ...   nan   nan   nan]\n",
      " [ 998.  997.  995. ...   nan   nan   nan]\n",
      " ...\n",
      " [1006. 1006. 1006. ...   nan   nan   nan]\n",
      " [1002. 1002. 1000. ...   nan   nan   nan]\n",
      " [1002. 1002. 1002. ...   nan   nan   nan]]\n",
      "[[  nan   nan   nan ...   nan   nan   nan]\n",
      " [1000. 1001.  999. ...   nan   nan   nan]\n",
      " [ 998.  997.  995. ...   nan   nan   nan]\n",
      " ...\n",
      " [  nan   nan   nan ...   nan   nan   nan]\n",
      " [  nan   nan   nan ...   nan   nan   nan]\n",
      " [  nan   nan  997. ...   nan   nan   nan]]\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34['wmo_pres'].data)\n",
    "print(ds_80_00_processed_34['wmo_pres'].isel(storm=np.arange(20)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (storm: 1817, date_time: 360)>\n",
      "array([['1980-01-01T00:00:00.000040448', '1980-01-01T06:00:00.000040448',\n",
      "        '1980-01-01T12:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT'],\n",
      "       ['1980-01-02T18:00:00.000040448', '1980-01-03T00:00:00.000040448',\n",
      "        '1980-01-03T06:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT'],\n",
      "       ['1980-01-04T12:00:00.000040448', '1980-01-04T18:00:00.000040448',\n",
      "        '1980-01-05T00:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT'],\n",
      "       ...,\n",
      "       ['2000-12-23T06:00:00.000040448', '2000-12-23T12:00:00.000040448',\n",
      "        '2000-12-23T18:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT'],\n",
      "       ['2000-12-28T18:00:00.000040448', '2000-12-29T00:00:00.000040448',\n",
      "        '2000-12-29T06:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT'],\n",
      "       ['2000-12-31T06:00:00.000040448', '2000-12-31T12:00:00.000040448',\n",
      "        '2000-12-31T18:00:00.000040448', ...,                           'NaT',\n",
      "                                  'NaT',                           'NaT']],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     (storm, date_time) datetime64[ns] 1980-01-01T00:00:00.000040448 ... NaT\n",
      "    lat      (storm, date_time) float32 -12.5 -11.914368 -11.5 ... nan nan nan\n",
      "    lon      (storm, date_time) float32 172.5 172.41243 172.5 ... nan nan nan\n",
      "Dimensions without coordinates: storm, date_time\n",
      "Attributes:\n",
      "    long_name:              time\n",
      "    standard_name:          time\n",
      "    description:            Nominally, time steps are 3 hourly, but can be mo...\n",
      "    Note:                   Variable:time can be missing since the tracks are...\n",
      "    coverage_content_type:  physicalMeasurement\n"
     ]
    }
   ],
   "source": [
    "print(ds_80_00_processed_34['usa_lat'].time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mod(usa_lat_temp['time.hour'].isel(date_time=1).data, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1980-01-01T00:00:00.000040448' '1980-01-01T06:00:00.000040448'\n",
      " '1980-01-01T12:00:00.000040448' '1980-01-01T18:00:00.000040448'\n",
      " '1980-01-02T00:00:00.000040448' '1980-01-02T06:00:00.000040448'\n",
      " '1980-01-02T12:00:00.000040448' '1980-01-02T18:00:00.000040448'\n",
      " '1980-01-03T00:00:00.000040448' '1980-01-03T06:00:00.000040448']\n",
      "['1980-01-02T18:00:00.000040448' '1980-01-03T00:00:00.000040448'\n",
      " '1980-01-03T06:00:00.000040448' '1980-01-03T12:00:00.000040448'\n",
      " '1980-01-03T18:00:00.000040448' '1980-01-04T00:00:00.000040448'\n",
      " '1980-01-04T06:00:00.000040448' '1980-01-04T12:00:00.000040448'\n",
      " '1980-01-04T18:00:00.000040448' '1980-01-05T00:00:00.000040448']\n",
      "['1980-01-04T12:00:00.000040448' '1980-01-04T18:00:00.000040448'\n",
      " '1980-01-05T00:00:00.000040448' '1980-01-05T06:00:00.000040448'\n",
      " '1980-01-05T12:00:00.000040448' '1980-01-05T18:00:00.000040448'\n",
      " '1980-01-06T00:00:00.000040448' '1980-01-06T06:00:00.000040448'\n",
      " '1980-01-06T12:00:00.000040448' '1980-01-06T18:00:00.000040448']\n",
      "['1980-01-15T06:00:00.000040448' '1980-01-15T12:00:00.000040448'\n",
      " '1980-01-15T18:00:00.000040448' '1980-01-16T00:00:00.000040448'\n",
      " '1980-01-16T06:00:00.000040448' '1980-01-16T12:00:00.000040448'\n",
      " '1980-01-16T18:00:00.000040448' '1980-01-17T00:00:00.000040448'\n",
      " '1980-01-17T06:00:00.000040448' '1980-01-17T12:00:00.000040448']\n",
      "['1980-01-18T00:00:00.000040448' '1980-01-18T06:00:00.000040448'\n",
      " '1980-01-18T12:00:00.000040448' '1980-01-18T18:00:00.000040448'\n",
      " '1980-01-19T00:00:00.000040448' '1980-01-19T06:00:00.000040448'\n",
      " '1980-01-19T12:00:00.000040448' '1980-01-19T18:00:00.000040448'\n",
      " '1980-01-20T00:00:00.000040448' '1980-01-20T06:00:00.000040448']\n",
      "['1980-01-27T00:00:00.000040448' '1980-01-27T06:00:00.000040448'\n",
      " '1980-01-27T12:00:00.000040448' '1980-01-27T18:00:00.000040448'\n",
      " '1980-01-28T00:00:00.000040448' '1980-01-28T06:00:00.000040448'\n",
      " '1980-01-28T12:00:00.000040448' '1980-01-28T18:00:00.000040448'\n",
      " '1980-01-29T00:00:00.000040448' '1980-01-29T06:00:00.000040448']\n",
      "['1980-02-01T06:00:00.000040448' '1980-02-01T12:00:00.000040448'\n",
      " '1980-02-01T18:00:00.000040448' '1980-02-02T00:00:00.000040448'\n",
      " '1980-02-02T06:00:00.000040448' '1980-02-02T12:00:00.000040448'\n",
      " '1980-02-02T18:00:00.000040448' '1980-02-03T00:00:00.000040448'\n",
      " '1980-02-03T06:00:00.000040448' '1980-02-03T12:00:00.000040448']\n",
      "['1980-02-02T00:00:00.000040448' '1980-02-02T06:00:00.000040448'\n",
      " '1980-02-02T12:00:00.000040448' '1980-02-02T18:00:00.000040448'\n",
      " '1980-02-03T00:00:00.000040448' '1980-02-03T06:00:00.000040448'\n",
      " '1980-02-03T12:00:00.000040448' '1980-02-03T18:00:00.000040448'\n",
      " '1980-02-04T00:00:00.000040448' '1980-02-04T06:00:00.000040448']\n",
      "['1980-02-11T00:00:00.000040448' '1980-02-11T06:00:00.000040448'\n",
      " '1980-02-11T12:00:00.000040448' '1980-02-11T18:00:00.000040448'\n",
      " '1980-02-12T00:00:00.000040448' '1980-02-12T06:00:00.000040448'\n",
      " '1980-02-12T12:00:00.000040448' '1980-02-12T18:00:00.000040448'\n",
      " '1980-02-13T00:00:00.000040448' '1980-02-13T06:00:00.000040448']\n",
      "['1980-02-12T00:00:00.000040448' '1980-02-12T06:00:00.000040448'\n",
      " '1980-02-12T12:00:00.000040448' '1980-02-12T18:00:00.000040448'\n",
      " '1980-02-13T00:00:00.000040448' '1980-02-13T06:00:00.000040448'\n",
      " '1980-02-13T12:00:00.000040448' '1980-02-13T18:00:00.000040448'\n",
      " '1980-02-14T00:00:00.000040448' '1980-02-14T06:00:00.000040448']\n",
      "['1980-02-20T00:00:00.000040448' '1980-02-20T06:00:00.000040448'\n",
      " '1980-02-20T12:00:00.000040448' '1980-02-20T18:00:00.000040448'\n",
      " '1980-02-21T00:00:00.000040448' '1980-02-21T06:00:00.000040448'\n",
      " '1980-02-21T12:00:00.000040448' '1980-02-21T18:00:00.000040448'\n",
      " '1980-02-22T00:00:00.000040448' '1980-02-22T06:00:00.000040448']\n",
      "['1980-02-21T00:00:00.000040448' '1980-02-21T06:00:00.000040448'\n",
      " '1980-02-21T12:00:00.000040448' '1980-02-21T18:00:00.000040448'\n",
      " '1980-02-22T00:00:00.000040448' '1980-02-22T06:00:00.000040448'\n",
      " '1980-02-22T12:00:00.000040448' '1980-02-22T18:00:00.000040448'\n",
      " '1980-02-23T00:00:00.000040448' '1980-02-23T06:00:00.000040448']\n",
      "['1980-02-25T06:00:00.000040448' '1980-02-25T12:00:00.000040448'\n",
      " '1980-02-25T18:00:00.000040448' '1980-02-26T00:00:00.000040448'\n",
      " '1980-02-26T06:00:00.000040448' '1980-02-26T12:00:00.000040448'\n",
      " '1980-02-26T18:00:00.000040448' '1980-02-27T00:00:00.000040448'\n",
      " '1980-02-27T06:00:00.000040448' '1980-02-27T12:00:00.000040448']\n",
      "['1980-03-08T00:00:00.000040448' '1980-03-08T06:00:00.000040448'\n",
      " '1980-03-08T12:00:00.000040448' '1980-03-08T18:00:00.000040448'\n",
      " '1980-03-09T00:00:00.000040448' '1980-03-09T06:00:00.000040448'\n",
      " '1980-03-09T12:00:00.000040448' '1980-03-09T18:00:00.000040448'\n",
      " '1980-03-10T00:00:00.000040448' '1980-03-10T06:00:00.000040448']\n",
      "['1980-03-09T00:00:00.000040448' '1980-03-09T06:00:00.000040448'\n",
      " '1980-03-09T12:00:00.000040448' '1980-03-09T18:00:00.000040448'\n",
      " '1980-03-10T00:00:00.000040448' '1980-03-10T06:00:00.000040448'\n",
      " '1980-03-10T12:00:00.000040448' '1980-03-10T18:00:00.000040448'\n",
      " '1980-03-11T00:00:00.000040448' '1980-03-11T06:00:00.000040448']\n",
      "['1980-03-12T18:00:00.000040448' '1980-03-13T00:00:00.000040448'\n",
      " '1980-03-13T06:00:00.000040448' '1980-03-13T12:00:00.000040448'\n",
      " '1980-03-13T18:00:00.000040448' '1980-03-14T00:00:00.000040448'\n",
      " '1980-03-14T06:00:00.000040448' '1980-03-14T12:00:00.000040448'\n",
      " '1980-03-14T18:00:00.000040448' '1980-03-15T00:00:00.000040448']\n",
      "['1980-03-14T12:00:00.000040448' '1980-03-14T18:00:00.000040448'\n",
      " '1980-03-15T00:00:00.000040448' '1980-03-15T06:00:00.000040448'\n",
      " '1980-03-15T12:00:00.000040448' '1980-03-15T18:00:00.000040448'\n",
      " '1980-03-16T00:00:00.000040448' '1980-03-16T06:00:00.000040448'\n",
      " '1980-03-16T12:00:00.000040448' '1980-03-16T18:00:00.000040448']\n",
      "['1980-03-21T00:00:00.000040448' '1980-03-21T06:00:00.000040448'\n",
      " '1980-03-21T12:00:00.000040448' '1980-03-21T18:00:00.000040448'\n",
      " '1980-03-22T00:00:00.000040448' '1980-03-22T06:00:00.000040448'\n",
      " '1980-03-22T12:00:00.000040448' '1980-03-22T18:00:00.000040448'\n",
      " '1980-03-23T00:00:00.000040448' '1980-03-23T06:00:00.000040448']\n",
      "['1980-03-21T12:00:00.000040448' '1980-03-21T18:00:00.000040448'\n",
      " '1980-03-22T00:00:00.000040448' '1980-03-22T06:00:00.000040448'\n",
      " '1980-03-22T12:00:00.000040448' '1980-03-22T18:00:00.000040448'\n",
      " '1980-03-23T00:00:00.000040448'                           'NaT'\n",
      "                           'NaT'                           'NaT']\n",
      "['1980-03-25T00:00:00.000040448' '1980-03-25T06:00:00.000040448'\n",
      " '1980-03-25T12:00:00.000040448' '1980-03-25T18:00:00.000040448'\n",
      " '1980-03-26T00:00:00.000040448' '1980-03-26T06:00:00.000040448'\n",
      " '1980-03-26T12:00:00.000040448' '1980-03-26T18:00:00.000040448'\n",
      " '1980-03-27T00:00:00.000040448' '1980-03-27T06:00:00.000040448']\n",
      "['1980-04-01T00:00:00.000040448' '1980-04-01T06:00:00.000040448'\n",
      " '1980-04-01T12:00:00.000040448' '1980-04-01T18:00:00.000040448'\n",
      " '1980-04-02T00:00:00.000040448' '1980-04-02T06:00:00.000040448'\n",
      " '1980-04-02T12:00:00.000040448' '1980-04-02T18:00:00.000040448'\n",
      " '1980-04-03T00:00:00.000040448' '1980-04-03T06:00:00.000040448']\n",
      "['1980-04-03T00:00:00.000040448' '1980-04-03T06:00:00.000040448'\n",
      " '1980-04-03T12:00:00.000040448' '1980-04-03T18:00:00.000040448'\n",
      " '1980-04-04T00:00:00.000040448' '1980-04-04T06:00:00.000040448'\n",
      " '1980-04-04T12:00:00.000040448' '1980-04-04T18:00:00.000040448'\n",
      " '1980-04-05T00:00:00.000040448' '1980-04-05T06:00:00.000040448']\n",
      "['1980-05-05T00:00:00.000040448' '1980-05-05T06:00:00.000040448'\n",
      " '1980-05-05T12:00:00.000040448' '1980-05-05T18:00:00.000040448'\n",
      " '1980-05-06T00:00:00.000040448' '1980-05-06T06:00:00.000040448'\n",
      " '1980-05-06T12:00:00.000040448' '1980-05-06T18:00:00.000040448'\n",
      " '1980-05-07T00:00:00.000040448' '1980-05-07T06:00:00.000040448']\n",
      "['1980-05-11T12:00:00.000040448' '1980-05-11T18:00:00.000040448'\n",
      " '1980-05-12T00:00:00.000040448' '1980-05-12T06:00:00.000040448'\n",
      " '1980-05-12T12:00:00.000040448' '1980-05-12T18:00:00.000040448'\n",
      " '1980-05-13T00:00:00.000040448' '1980-05-13T06:00:00.000040448'\n",
      " '1980-05-13T12:00:00.000040448' '1980-05-13T18:00:00.000040448']\n",
      "['1980-05-16T00:00:00.000040448' '1980-05-16T06:00:00.000040448'\n",
      " '1980-05-16T12:00:00.000040448' '1980-05-16T18:00:00.000040448'\n",
      " '1980-05-17T00:00:00.000040448' '1980-05-17T06:00:00.000040448'\n",
      " '1980-05-17T12:00:00.000040448' '1980-05-17T18:00:00.000040448'\n",
      " '1980-05-18T00:00:00.000040448' '1980-05-18T06:00:00.000040448']\n",
      "['1980-05-19T00:00:00.000040448' '1980-05-19T06:00:00.000040448'\n",
      " '1980-05-19T12:00:00.000040448' '1980-05-19T18:00:00.000040448'\n",
      " '1980-05-20T00:00:00.000040448' '1980-05-20T06:00:00.000040448'\n",
      " '1980-05-20T12:00:00.000040448' '1980-05-20T18:00:00.000040448'\n",
      " '1980-05-21T00:00:00.000040448' '1980-05-21T06:00:00.000040448']\n",
      "['1980-06-09T00:00:00.000040448' '1980-06-09T06:00:00.000040448'\n",
      " '1980-06-09T12:00:00.000040448' '1980-06-09T18:00:00.000040448'\n",
      " '1980-06-10T00:00:00.000040448' '1980-06-10T06:00:00.000040448'\n",
      " '1980-06-10T12:00:00.000040448' '1980-06-10T18:00:00.000040448'\n",
      " '1980-06-11T00:00:00.000040448' '1980-06-11T06:00:00.000040448']\n",
      "['1980-06-16T18:00:00.000040448' '1980-06-17T00:00:00.000040448'\n",
      " '1980-06-17T06:00:00.000040448' '1980-06-17T12:00:00.000040448'\n",
      " '1980-06-17T18:00:00.000040448' '1980-06-18T00:00:00.000040448'\n",
      " '1980-06-18T06:00:00.000040448' '1980-06-18T12:00:00.000040448'\n",
      " '1980-06-18T18:00:00.000040448' '1980-06-19T00:00:00.000040448']\n",
      "['1980-06-19T00:00:00.000040448' '1980-06-19T06:00:00.000040448'\n",
      " '1980-06-19T12:00:00.000040448' '1980-06-19T18:00:00.000040448'\n",
      " '1980-06-20T00:00:00.000040448' '1980-06-20T06:00:00.000040448'\n",
      " '1980-06-20T12:00:00.000040448' '1980-06-20T18:00:00.000040448'\n",
      " '1980-06-21T00:00:00.000040448' '1980-06-21T06:00:00.000040448']\n",
      "['1980-06-25T06:00:00.000040448' '1980-06-25T12:00:00.000040448'\n",
      " '1980-06-25T18:00:00.000040448' '1980-06-26T00:00:00.000040448'\n",
      " '1980-06-26T06:00:00.000040448' '1980-06-26T12:00:00.000040448'\n",
      " '1980-06-26T18:00:00.000040448' '1980-06-27T00:00:00.000040448'\n",
      " '1980-06-27T06:00:00.000040448' '1980-06-27T12:00:00.000040448']\n",
      "['1980-07-01T00:00:00.000040448' '1980-07-01T06:00:00.000040448'\n",
      " '1980-07-01T12:00:00.000040448' '1980-07-01T18:00:00.000040448'\n",
      " '1980-07-02T00:00:00.000040448' '1980-07-02T06:00:00.000040448'\n",
      " '1980-07-02T12:00:00.000040448' '1980-07-02T18:00:00.000040448'\n",
      " '1980-07-03T00:00:00.000040448' '1980-07-03T06:00:00.000040448']\n",
      "['1980-07-05T00:00:00.000040448' '1980-07-05T06:00:00.000040448'\n",
      " '1980-07-05T12:00:00.000040448' '1980-07-05T18:00:00.000040448'\n",
      " '1980-07-06T00:00:00.000040448' '1980-07-06T06:00:00.000040448'\n",
      " '1980-07-06T12:00:00.000040448' '1980-07-06T18:00:00.000040448'\n",
      " '1980-07-07T00:00:00.000040448' '1980-07-07T06:00:00.000040448']\n",
      "['1980-07-12T00:00:00.000040448' '1980-07-12T06:00:00.000040448'\n",
      " '1980-07-12T12:00:00.000040448' '1980-07-12T18:00:00.000040448'\n",
      " '1980-07-13T00:00:00.000040448'                           'NaT'\n",
      "                           'NaT'                           'NaT'\n",
      "                           'NaT'                           'NaT']\n",
      "['1980-07-15T00:00:00.000040448' '1980-07-15T06:00:00.000040448'\n",
      " '1980-07-15T12:00:00.000040448' '1980-07-15T18:00:00.000040448'\n",
      " '1980-07-16T00:00:00.000040448' '1980-07-16T06:00:00.000040448'\n",
      " '1980-07-16T12:00:00.000040448' '1980-07-16T18:00:00.000040448'\n",
      " '1980-07-17T00:00:00.000040448' '1980-07-17T06:00:00.000040448']\n",
      "['1980-07-18T06:00:00.000040448' '1980-07-18T12:00:00.000040448'\n",
      " '1980-07-18T18:00:00.000040448' '1980-07-19T00:00:00.000040448'\n",
      " '1980-07-19T06:00:00.000040448' '1980-07-19T12:00:00.000040448'\n",
      " '1980-07-19T18:00:00.000040448' '1980-07-20T00:00:00.000040448'\n",
      " '1980-07-20T06:00:00.000040448' '1980-07-20T12:00:00.000040448']\n",
      "['1980-07-19T00:00:00.000040448' '1980-07-19T06:00:00.000040448'\n",
      " '1980-07-19T12:00:00.000040448' '1980-07-19T18:00:00.000040448'\n",
      " '1980-07-20T00:00:00.000040448' '1980-07-20T06:00:00.000040448'\n",
      " '1980-07-20T12:00:00.000040448' '1980-07-20T18:00:00.000040448'\n",
      " '1980-07-21T00:00:00.000040448' '1980-07-21T06:00:00.000040448']\n",
      "['1980-07-26T06:00:00.000040448' '1980-07-26T12:00:00.000040448'\n",
      " '1980-07-26T18:00:00.000040448' '1980-07-27T00:00:00.000040448'\n",
      " '1980-07-27T06:00:00.000040448' '1980-07-27T12:00:00.000040448'\n",
      " '1980-07-27T18:00:00.000040448' '1980-07-28T00:00:00.000040448'\n",
      " '1980-07-28T06:00:00.000040448' '1980-07-28T12:00:00.000040448']\n",
      "['1980-07-28T06:00:00.000040448' '1980-07-28T12:00:00.000040448'\n",
      " '1980-07-28T18:00:00.000040448' '1980-07-29T00:00:00.000040448'\n",
      " '1980-07-29T06:00:00.000040448' '1980-07-29T12:00:00.000040448'\n",
      " '1980-07-29T18:00:00.000040448' '1980-07-30T00:00:00.000040448'\n",
      " '1980-07-30T06:00:00.000040448' '1980-07-30T12:00:00.000040448']\n",
      "['1980-07-31T00:00:00.000040448' '1980-07-31T06:00:00.000040448'\n",
      " '1980-07-31T12:00:00.000040448' '1980-07-31T18:00:00.000040448'\n",
      " '1980-08-01T00:00:00.000040448' '1980-08-01T06:00:00.000040448'\n",
      " '1980-08-01T12:00:00.000040448' '1980-08-01T18:00:00.000040448'\n",
      " '1980-08-02T00:00:00.000040448' '1980-08-02T06:00:00.000040448']\n",
      "['1980-07-31T12:00:00.000040448' '1980-07-31T18:00:00.000040448'\n",
      " '1980-08-01T00:00:00.000040448' '1980-08-01T06:00:00.000040448'\n",
      " '1980-08-01T12:00:00.000040448' '1980-08-01T18:00:00.000040448'\n",
      " '1980-08-02T00:00:00.000040448' '1980-08-02T06:00:00.000040448'\n",
      " '1980-08-02T12:00:00.000040448' '1980-08-02T18:00:00.000040448']\n",
      "['1980-08-06T00:00:00.000040448' '1980-08-06T06:00:00.000040448'\n",
      " '1980-08-06T12:00:00.000040448' '1980-08-06T18:00:00.000040448'\n",
      " '1980-08-07T00:00:00.000040448' '1980-08-07T06:00:00.000040448'\n",
      " '1980-08-07T12:00:00.000040448' '1980-08-07T18:00:00.000040448'\n",
      " '1980-08-08T00:00:00.000040448' '1980-08-08T06:00:00.000040448']\n",
      "['1980-08-05T18:00:00.000040448' '1980-08-06T00:00:00.000040448'\n",
      " '1980-08-06T06:00:00.000040448' '1980-08-06T12:00:00.000040448'\n",
      " '1980-08-06T18:00:00.000040448' '1980-08-07T00:00:00.000040448'\n",
      " '1980-08-07T06:00:00.000040448' '1980-08-07T12:00:00.000040448'\n",
      " '1980-08-07T18:00:00.000040448' '1980-08-08T00:00:00.000040448']\n",
      "['1980-08-14T00:00:00.000040448' '1980-08-14T06:00:00.000040448'\n",
      " '1980-08-14T12:00:00.000040448' '1980-08-14T18:00:00.000040448'\n",
      " '1980-08-15T00:00:00.000040448' '1980-08-15T06:00:00.000040448'\n",
      " '1980-08-15T12:00:00.000040448' '1980-08-15T18:00:00.000040448'\n",
      " '1980-08-16T00:00:00.000040448' '1980-08-16T06:00:00.000040448']\n",
      "['1980-08-20T12:00:00.000040448' '1980-08-20T18:00:00.000040448'\n",
      " '1980-08-21T00:00:00.000040448' '1980-08-21T06:00:00.000040448'\n",
      " '1980-08-21T12:00:00.000040448' '1980-08-21T18:00:00.000040448'\n",
      " '1980-08-22T00:00:00.000040448' '1980-08-22T06:00:00.000040448'\n",
      " '1980-08-22T12:00:00.000040448' '1980-08-22T18:00:00.000040448']\n",
      "['1980-08-22T18:00:00.000040448' '1980-08-23T00:00:00.000040448'\n",
      " '1980-08-23T06:00:00.000040448' '1980-08-23T12:00:00.000040448'\n",
      " '1980-08-23T18:00:00.000040448' '1980-08-24T00:00:00.000040448'\n",
      " '1980-08-24T06:00:00.000040448' '1980-08-24T12:00:00.000040448'\n",
      " '1980-08-24T18:00:00.000040448' '1980-08-25T00:00:00.000040448']\n",
      "['1980-08-23T00:00:00.000040448' '1980-08-23T06:00:00.000040448'\n",
      " '1980-08-23T12:00:00.000040448' '1980-08-23T18:00:00.000040448'\n",
      " '1980-08-24T00:00:00.000040448' '1980-08-24T06:00:00.000040448'\n",
      " '1980-08-24T12:00:00.000040448' '1980-08-24T18:00:00.000040448'\n",
      " '1980-08-25T00:00:00.000040448' '1980-08-25T06:00:00.000040448']\n",
      "['1980-09-01T00:00:00.000040448' '1980-09-01T06:00:00.000040448'\n",
      " '1980-09-01T12:00:00.000040448' '1980-09-01T18:00:00.000040448'\n",
      " '1980-09-02T00:00:00.000040448' '1980-09-02T06:00:00.000040448'\n",
      " '1980-09-02T12:00:00.000040448' '1980-09-02T18:00:00.000040448'\n",
      " '1980-09-03T00:00:00.000040448' '1980-09-03T06:00:00.000040448']\n",
      "['1980-09-01T18:00:00.000040448' '1980-09-02T00:00:00.000040448'\n",
      " '1980-09-02T06:00:00.000040448' '1980-09-02T12:00:00.000040448'\n",
      " '1980-09-02T18:00:00.000040448' '1980-09-03T00:00:00.000040448'\n",
      " '1980-09-03T06:00:00.000040448' '1980-09-03T12:00:00.000040448'\n",
      " '1980-09-03T18:00:00.000040448' '1980-09-04T00:00:00.000040448']\n",
      "['1980-09-04T12:00:00.000040448' '1980-09-04T18:00:00.000040448'\n",
      " '1980-09-05T00:00:00.000040448' '1980-09-05T06:00:00.000040448'\n",
      " '1980-09-05T12:00:00.000040448' '1980-09-05T18:00:00.000040448'\n",
      " '1980-09-06T00:00:00.000040448' '1980-09-06T06:00:00.000040448'\n",
      " '1980-09-06T12:00:00.000040448' '1980-09-06T18:00:00.000040448']\n",
      "['1980-09-04T18:00:00.000040448' '1980-09-05T00:00:00.000040448'\n",
      " '1980-09-05T06:00:00.000040448' '1980-09-05T12:00:00.000040448'\n",
      " '1980-09-05T18:00:00.000040448' '1980-09-06T00:00:00.000040448'\n",
      " '1980-09-06T06:00:00.000040448' '1980-09-06T12:00:00.000040448'\n",
      " '1980-09-06T18:00:00.000040448' '1980-09-07T00:00:00.000040448']\n",
      "['1980-09-06T00:00:00.000040448' '1980-09-06T06:00:00.000040448'\n",
      " '1980-09-06T12:00:00.000040448' '1980-09-06T18:00:00.000040448'\n",
      " '1980-09-07T00:00:00.000040448' '1980-09-07T06:00:00.000040448'\n",
      " '1980-09-07T12:00:00.000040448' '1980-09-07T18:00:00.000040448'\n",
      " '1980-09-08T00:00:00.000040448' '1980-09-08T06:00:00.000040448']\n",
      "['1980-09-10T00:00:00.000040448' '1980-09-10T06:00:00.000040448'\n",
      " '1980-09-10T12:00:00.000040448' '1980-09-10T18:00:00.000040448'\n",
      " '1980-09-11T00:00:00.000040448' '1980-09-11T06:00:00.000040448'\n",
      " '1980-09-11T12:00:00.000040448' '1980-09-11T18:00:00.000040448'\n",
      " '1980-09-12T00:00:00.000040448' '1980-09-12T06:00:00.000040448']\n",
      "['1980-09-12T06:00:00.000040448' '1980-09-12T12:00:00.000040448'\n",
      " '1980-09-12T18:00:00.000040448' '1980-09-13T00:00:00.000040448'\n",
      " '1980-09-13T06:00:00.000040448' '1980-09-13T12:00:00.000040448'\n",
      " '1980-09-13T18:00:00.000040448' '1980-09-14T00:00:00.000040448'\n",
      " '1980-09-14T06:00:00.000040448' '1980-09-14T12:00:00.000040448']\n",
      "['1980-09-13T06:00:00.000040448' '1980-09-13T12:00:00.000040448'\n",
      " '1980-09-13T18:00:00.000040448' '1980-09-14T00:00:00.000040448'\n",
      " '1980-09-14T06:00:00.000040448' '1980-09-14T12:00:00.000040448'\n",
      " '1980-09-14T18:00:00.000040448' '1980-09-15T00:00:00.000040448'\n",
      " '1980-09-15T06:00:00.000040448' '1980-09-15T12:00:00.000040448']\n",
      "['1980-09-16T06:00:00.000040448' '1980-09-16T12:00:00.000040448'\n",
      " '1980-09-16T18:00:00.000040448' '1980-09-17T00:00:00.000040448'\n",
      " '1980-09-17T06:00:00.000040448' '1980-09-17T12:00:00.000040448'\n",
      " '1980-09-17T18:00:00.000040448' '1980-09-18T00:00:00.000040448'\n",
      " '1980-09-18T06:00:00.000040448' '1980-09-18T12:00:00.000040448']\n",
      "['1980-09-20T12:00:00.000040448' '1980-09-20T18:00:00.000040448'\n",
      " '1980-09-21T00:00:00.000040448' '1980-09-21T06:00:00.000040448'\n",
      " '1980-09-21T12:00:00.000040448' '1980-09-21T18:00:00.000040448'\n",
      " '1980-09-22T00:00:00.000040448' '1980-09-22T06:00:00.000040448'\n",
      " '1980-09-22T12:00:00.000040448' '1980-09-22T18:00:00.000040448']\n",
      "['1980-09-21T18:00:00.000040448' '1980-09-22T00:00:00.000040448'\n",
      " '1980-09-22T06:00:00.000040448' '1980-09-22T12:00:00.000040448'\n",
      " '1980-09-22T18:00:00.000040448' '1980-09-23T00:00:00.000040448'\n",
      " '1980-09-23T06:00:00.000040448' '1980-09-23T12:00:00.000040448'\n",
      " '1980-09-23T18:00:00.000040448' '1980-09-24T00:00:00.000040448']\n",
      "['1980-09-25T00:00:00.000040448' '1980-09-25T06:00:00.000040448'\n",
      " '1980-09-25T12:00:00.000040448' '1980-09-25T18:00:00.000040448'\n",
      " '1980-09-26T00:00:00.000040448' '1980-09-26T06:00:00.000040448'\n",
      " '1980-09-26T12:00:00.000040448' '1980-09-26T18:00:00.000040448'\n",
      " '1980-09-27T00:00:00.000040448' '1980-09-27T06:00:00.000040448']\n",
      "['1980-09-26T00:00:00.000040448' '1980-09-26T06:00:00.000040448'\n",
      " '1980-09-26T12:00:00.000040448' '1980-09-26T18:00:00.000040448'\n",
      " '1980-09-27T00:00:00.000040448' '1980-09-27T06:00:00.000040448'\n",
      " '1980-09-27T12:00:00.000040448' '1980-09-27T18:00:00.000040448'\n",
      " '1980-09-28T00:00:00.000040448' '1980-09-28T06:00:00.000040448']\n",
      "['1980-10-01T00:00:00.000040448' '1980-10-01T06:00:00.000040448'\n",
      " '1980-10-01T12:00:00.000040448' '1980-10-01T18:00:00.000040448'\n",
      " '1980-10-02T00:00:00.000040448' '1980-10-02T06:00:00.000040448'\n",
      " '1980-10-02T12:00:00.000040448' '1980-10-02T18:00:00.000040448'\n",
      " '1980-10-03T00:00:00.000040448' '1980-10-03T06:00:00.000040448']\n",
      "['1980-10-02T12:00:00.000040448' '1980-10-02T18:00:00.000040448'\n",
      " '1980-10-03T00:00:00.000040448' '1980-10-03T06:00:00.000040448'\n",
      " '1980-10-03T12:00:00.000040448' '1980-10-03T18:00:00.000040448'\n",
      " '1980-10-04T00:00:00.000040448' '1980-10-04T06:00:00.000040448'\n",
      " '1980-10-04T12:00:00.000040448' '1980-10-04T18:00:00.000040448']\n",
      "['1980-10-08T06:00:00.000040448' '1980-10-08T12:00:00.000040448'\n",
      " '1980-10-08T18:00:00.000040448' '1980-10-09T00:00:00.000040448'\n",
      " '1980-10-09T06:00:00.000040448' '1980-10-09T12:00:00.000040448'\n",
      " '1980-10-09T18:00:00.000040448' '1980-10-10T00:00:00.000040448'\n",
      " '1980-10-10T06:00:00.000040448' '1980-10-10T12:00:00.000040448']\n",
      "['1980-10-10T06:00:00.000040448' '1980-10-10T12:00:00.000040448'\n",
      " '1980-10-10T18:00:00.000040448' '1980-10-11T00:00:00.000040448'\n",
      " '1980-10-11T06:00:00.000040448' '1980-10-11T12:00:00.000040448'\n",
      " '1980-10-11T18:00:00.000040448' '1980-10-12T00:00:00.000040448'\n",
      " '1980-10-12T06:00:00.000040448' '1980-10-12T12:00:00.000040448']\n",
      "['1980-10-11T00:00:00.000040448' '1980-10-11T06:00:00.000040448'\n",
      " '1980-10-11T12:00:00.000040448' '1980-10-11T18:00:00.000040448'\n",
      " '1980-10-12T00:00:00.000040448' '1980-10-12T06:00:00.000040448'\n",
      "                           'NaT'                           'NaT'\n",
      "                           'NaT'                           'NaT']\n",
      "['1980-10-22T00:00:00.000040448' '1980-10-22T06:00:00.000040448'\n",
      " '1980-10-22T12:00:00.000040448' '1980-10-22T18:00:00.000040448'\n",
      " '1980-10-23T00:00:00.000040448' '1980-10-23T06:00:00.000040448'\n",
      " '1980-10-23T12:00:00.000040448' '1980-10-23T18:00:00.000040448'\n",
      " '1980-10-24T00:00:00.000040448' '1980-10-24T06:00:00.000040448']\n",
      "['1980-10-26T00:00:00.000040448' '1980-10-26T06:00:00.000040448'\n",
      " '1980-10-26T12:00:00.000040448' '1980-10-26T18:00:00.000040448'\n",
      " '1980-10-27T00:00:00.000040448' '1980-10-27T06:00:00.000040448'\n",
      " '1980-10-27T12:00:00.000040448' '1980-10-27T18:00:00.000040448'\n",
      " '1980-10-28T00:00:00.000040448' '1980-10-28T06:00:00.000040448']\n",
      "['1980-10-28T06:00:00.000040448' '1980-10-28T12:00:00.000040448'\n",
      " '1980-10-28T18:00:00.000040448' '1980-10-29T00:00:00.000040448'\n",
      " '1980-10-29T06:00:00.000040448' '1980-10-29T12:00:00.000040448'\n",
      "                           'NaT'                           'NaT'\n",
      "                           'NaT'                           'NaT']\n",
      "['1980-11-07T18:00:00.000040448' '1980-11-08T00:00:00.000040448'\n",
      " '1980-11-08T06:00:00.000040448' '1980-11-08T12:00:00.000040448'\n",
      " '1980-11-08T18:00:00.000040448' '1980-11-09T00:00:00.000040448'\n",
      " '1980-11-09T06:00:00.000040448' '1980-11-09T12:00:00.000040448'\n",
      " '1980-11-09T18:00:00.000040448' '1980-11-10T00:00:00.000040448']\n",
      "['1980-11-12T06:00:00.000040448' '1980-11-12T12:00:00.000040448'\n",
      " '1980-11-12T18:00:00.000040448' '1980-11-13T00:00:00.000040448'\n",
      " '1980-11-13T06:00:00.000040448' '1980-11-13T12:00:00.000040448'\n",
      " '1980-11-13T18:00:00.000040448' '1980-11-14T00:00:00.000040448'\n",
      " '1980-11-14T06:00:00.000040448' '1980-11-14T12:00:00.000040448']\n",
      "['1980-11-18T06:00:00.000040448' '1980-11-18T12:00:00.000040448'\n",
      " '1980-11-18T18:00:00.000040448' '1980-11-19T00:00:00.000040448'\n",
      " '1980-11-19T06:00:00.000040448' '1980-11-19T12:00:00.000040448'\n",
      " '1980-11-19T18:00:00.000040448' '1980-11-20T00:00:00.000040448'\n",
      " '1980-11-20T06:00:00.000040448' '1980-11-20T12:00:00.000040448']\n",
      "['1980-11-25T00:00:00.000040448' '1980-11-25T06:00:00.000040448'\n",
      " '1980-11-25T12:00:00.000040448' '1980-11-25T18:00:00.000040448'\n",
      " '1980-11-26T00:00:00.000040448' '1980-11-26T06:00:00.000040448'\n",
      " '1980-11-26T12:00:00.000040448' '1980-11-26T18:00:00.000040448'\n",
      " '1980-11-27T00:00:00.000040448' '1980-11-27T06:00:00.000040448']\n",
      "['1980-12-10T12:00:00.000040448' '1980-12-10T18:00:00.000040448'\n",
      " '1980-12-11T00:00:00.000040448' '1980-12-11T06:00:00.000040448'\n",
      " '1980-12-11T12:00:00.000040448' '1980-12-11T18:00:00.000040448'\n",
      " '1980-12-12T00:00:00.000040448' '1980-12-12T06:00:00.000040448'\n",
      " '1980-12-12T12:00:00.000040448' '1980-12-12T18:00:00.000040448']\n",
      "['1980-12-14T00:00:00.000040448' '1980-12-14T06:00:00.000040448'\n",
      " '1980-12-14T12:00:00.000040448' '1980-12-14T18:00:00.000040448'\n",
      " '1980-12-15T00:00:00.000040448' '1980-12-15T06:00:00.000040448'\n",
      " '1980-12-15T12:00:00.000040448' '1980-12-15T18:00:00.000040448'\n",
      " '1980-12-16T00:00:00.000040448' '1980-12-16T06:00:00.000040448']\n",
      "['1981-01-03T06:00:00.000040448' '1981-01-03T12:00:00.000040448'\n",
      " '1981-01-03T18:00:00.000040448' '1981-01-04T00:00:00.000040448'\n",
      " '1981-01-04T06:00:00.000040448' '1981-01-04T12:00:00.000040448'\n",
      " '1981-01-04T18:00:00.000040448' '1981-01-05T00:00:00.000040448'\n",
      " '1981-01-05T06:00:00.000040448' '1981-01-05T12:00:00.000040448']\n",
      "['1981-01-11T00:00:00.000040448' '1981-01-11T06:00:00.000040448'\n",
      " '1981-01-11T12:00:00.000040448' '1981-01-11T18:00:00.000040448'\n",
      " '1981-01-12T00:00:00.000040448' '1981-01-12T06:00:00.000040448'\n",
      " '1981-01-12T12:00:00.000040448' '1981-01-12T18:00:00.000040448'\n",
      " '1981-01-13T00:00:00.000040448' '1981-01-13T06:00:00.000040448']\n",
      "['1981-01-12T00:00:00.000040448' '1981-01-12T06:00:00.000040448'\n",
      " '1981-01-12T12:00:00.000040448' '1981-01-12T18:00:00.000040448'\n",
      " '1981-01-13T00:00:00.000040448' '1981-01-13T06:00:00.000040448'\n",
      " '1981-01-13T12:00:00.000040448' '1981-01-13T18:00:00.000040448'\n",
      " '1981-01-14T00:00:00.000040448' '1981-01-14T06:00:00.000040448']\n",
      "['1981-01-14T12:00:00.000040448' '1981-01-14T18:00:00.000040448'\n",
      " '1981-01-15T00:00:00.000040448' '1981-01-15T06:00:00.000040448'\n",
      " '1981-01-15T12:00:00.000040448' '1981-01-15T18:00:00.000040448'\n",
      " '1981-01-16T00:00:00.000040448' '1981-01-16T06:00:00.000040448'\n",
      " '1981-01-16T12:00:00.000040448' '1981-01-16T18:00:00.000040448']\n",
      "['1981-01-28T00:00:00.000040448' '1981-01-28T06:00:00.000040448'\n",
      " '1981-01-28T12:00:00.000040448' '1981-01-28T18:00:00.000040448'\n",
      " '1981-01-29T00:00:00.000040448' '1981-01-29T06:00:00.000040448'\n",
      " '1981-01-29T12:00:00.000040448' '1981-01-29T18:00:00.000040448'\n",
      " '1981-01-30T00:00:00.000040448' '1981-01-30T06:00:00.000040448']\n",
      "['1981-02-08T00:00:00.000040448' '1981-02-08T06:00:00.000040448'\n",
      " '1981-02-08T12:00:00.000040448' '1981-02-08T18:00:00.000040448'\n",
      " '1981-02-09T00:00:00.000040448' '1981-02-09T06:00:00.000040448'\n",
      " '1981-02-09T12:00:00.000040448' '1981-02-09T18:00:00.000040448'\n",
      " '1981-02-10T00:00:00.000040448' '1981-02-10T06:00:00.000040448']\n",
      "['1981-02-08T06:00:00.000040448' '1981-02-08T12:00:00.000040448'\n",
      " '1981-02-08T18:00:00.000040448' '1981-02-09T00:00:00.000040448'\n",
      " '1981-02-09T06:00:00.000040448' '1981-02-09T12:00:00.000040448'\n",
      " '1981-02-09T18:00:00.000040448' '1981-02-10T00:00:00.000040448'\n",
      " '1981-02-10T06:00:00.000040448' '1981-02-10T12:00:00.000040448']\n",
      "['1981-02-16T12:00:00.000040448' '1981-02-16T18:00:00.000040448'\n",
      " '1981-02-17T00:00:00.000040448' '1981-02-17T06:00:00.000040448'\n",
      " '1981-02-17T12:00:00.000040448' '1981-02-17T18:00:00.000040448'\n",
      " '1981-02-18T00:00:00.000040448' '1981-02-18T06:00:00.000040448'\n",
      " '1981-02-18T12:00:00.000040448' '1981-02-18T18:00:00.000040448']\n",
      "['1981-02-24T00:00:00.000040448' '1981-02-24T06:00:00.000040448'\n",
      " '1981-02-24T12:00:00.000040448' '1981-02-24T18:00:00.000040448'\n",
      " '1981-02-25T00:00:00.000040448' '1981-02-25T06:00:00.000040448'\n",
      " '1981-02-25T12:00:00.000040448' '1981-02-25T18:00:00.000040448'\n",
      " '1981-02-26T00:00:00.000040448' '1981-02-26T06:00:00.000040448']\n",
      "['1981-02-24T18:00:00.000040448' '1981-02-25T00:00:00.000040448'\n",
      " '1981-02-25T06:00:00.000040448' '1981-02-25T12:00:00.000040448'\n",
      " '1981-02-25T18:00:00.000040448' '1981-02-26T00:00:00.000040448'\n",
      " '1981-02-26T06:00:00.000040448' '1981-02-26T12:00:00.000040448'\n",
      " '1981-02-26T18:00:00.000040448' '1981-02-27T00:00:00.000040448']\n",
      "['1981-03-01T06:00:00.000040448' '1981-03-01T12:00:00.000040448'\n",
      " '1981-03-01T18:00:00.000040448' '1981-03-02T00:00:00.000040448'\n",
      " '1981-03-02T06:00:00.000040448' '1981-03-02T12:00:00.000040448'\n",
      " '1981-03-02T18:00:00.000040448' '1981-03-03T00:00:00.000040448'\n",
      " '1981-03-03T06:00:00.000040448' '1981-03-03T12:00:00.000040448']\n",
      "['1981-03-01T12:00:00.000040448' '1981-03-01T18:00:00.000040448'\n",
      " '1981-03-02T00:00:00.000040448' '1981-03-02T06:00:00.000040448'\n",
      " '1981-03-02T12:00:00.000040448' '1981-03-02T18:00:00.000040448'\n",
      " '1981-03-03T00:00:00.000040448' '1981-03-03T06:00:00.000040448'\n",
      " '1981-03-03T12:00:00.000040448' '1981-03-03T18:00:00.000040448']\n",
      "['1981-03-10T00:00:00.000040448' '1981-03-10T06:00:00.000040448'\n",
      " '1981-03-10T12:00:00.000040448' '1981-03-10T18:00:00.000040448'\n",
      " '1981-03-11T00:00:00.000040448' '1981-03-11T06:00:00.000040448'\n",
      " '1981-03-11T12:00:00.000040448' '1981-03-11T18:00:00.000040448'\n",
      " '1981-03-12T00:00:00.000040448' '1981-03-12T06:00:00.000040448']\n",
      "['1981-03-11T00:00:00.000040448' '1981-03-11T06:00:00.000040448'\n",
      " '1981-03-11T12:00:00.000040448' '1981-03-11T18:00:00.000040448'\n",
      " '1981-03-12T00:00:00.000040448' '1981-03-12T06:00:00.000040448'\n",
      " '1981-03-12T12:00:00.000040448' '1981-03-12T18:00:00.000040448'\n",
      " '1981-03-13T00:00:00.000040448' '1981-03-13T06:00:00.000040448']\n",
      "['1981-03-28T00:00:00.000040448' '1981-03-28T06:00:00.000040448'\n",
      " '1981-03-28T12:00:00.000040448' '1981-03-28T18:00:00.000040448'\n",
      " '1981-03-29T00:00:00.000040448' '1981-03-29T06:00:00.000040448'\n",
      " '1981-03-29T12:00:00.000040448' '1981-03-29T18:00:00.000040448'\n",
      " '1981-03-30T00:00:00.000040448' '1981-03-30T06:00:00.000040448']\n",
      "['1981-04-04T00:00:00.000040448' '1981-04-04T06:00:00.000040448'\n",
      " '1981-04-04T12:00:00.000040448' '1981-04-04T18:00:00.000040448'\n",
      " '1981-04-05T00:00:00.000040448' '1981-04-05T06:00:00.000040448'\n",
      " '1981-04-05T12:00:00.000040448' '1981-04-05T18:00:00.000040448'\n",
      " '1981-04-06T00:00:00.000040448' '1981-04-06T06:00:00.000040448']\n",
      "['1981-04-06T00:00:00.000040448' '1981-04-06T06:00:00.000040448'\n",
      " '1981-04-06T12:00:00.000040448' '1981-04-06T18:00:00.000040448'\n",
      " '1981-04-07T00:00:00.000040448' '1981-04-07T06:00:00.000040448'\n",
      " '1981-04-07T12:00:00.000040448' '1981-04-07T18:00:00.000040448'\n",
      " '1981-04-08T00:00:00.000040448' '1981-04-08T06:00:00.000040448']\n",
      "['1981-04-12T00:00:00.000040448' '1981-04-12T06:00:00.000040448'\n",
      " '1981-04-12T12:00:00.000040448' '1981-04-12T18:00:00.000040448'\n",
      " '1981-04-13T00:00:00.000040448' '1981-04-13T06:00:00.000040448'\n",
      " '1981-04-13T12:00:00.000040448' '1981-04-13T18:00:00.000040448'\n",
      " '1981-04-14T00:00:00.000040448' '1981-04-14T06:00:00.000040448']\n",
      "['1981-04-28T00:00:00.000040448' '1981-04-28T06:00:00.000040448'\n",
      " '1981-04-28T12:00:00.000040448' '1981-04-28T18:00:00.000040448'\n",
      " '1981-04-29T00:00:00.000040448' '1981-04-29T06:00:00.000040448'\n",
      " '1981-04-29T12:00:00.000040448' '1981-04-29T18:00:00.000040448'\n",
      " '1981-04-30T00:00:00.000040448' '1981-04-30T06:00:00.000040448']\n",
      "['1981-05-06T18:00:00.000040448' '1981-05-07T00:00:00.000040448'\n",
      " '1981-05-07T06:00:00.000040448' '1981-05-07T12:00:00.000040448'\n",
      " '1981-05-07T18:00:00.000040448' '1981-05-08T00:00:00.000040448'\n",
      " '1981-05-08T06:00:00.000040448' '1981-05-08T12:00:00.000040448'\n",
      " '1981-05-08T18:00:00.000040448' '1981-05-09T00:00:00.000040448']\n",
      "['1981-05-22T00:00:00.000040448' '1981-05-22T06:00:00.000040448'\n",
      " '1981-05-22T12:00:00.000040448' '1981-05-22T18:00:00.000040448'\n",
      " '1981-05-23T00:00:00.000040448' '1981-05-23T06:00:00.000040448'\n",
      " '1981-05-23T12:00:00.000040448' '1981-05-23T18:00:00.000040448'\n",
      " '1981-05-24T00:00:00.000040448' '1981-05-24T06:00:00.000040448']\n",
      "['1981-05-30T18:00:00.000040448' '1981-05-31T00:00:00.000040448'\n",
      " '1981-05-31T06:00:00.000040448' '1981-05-31T12:00:00.000040448'\n",
      " '1981-05-31T18:00:00.000040448' '1981-06-01T00:00:00.000040448'\n",
      " '1981-06-01T06:00:00.000040448' '1981-06-01T12:00:00.000040448'\n",
      " '1981-06-01T18:00:00.000040448' '1981-06-02T00:00:00.000040448']\n",
      "['1981-06-07T00:00:00.000040448' '1981-06-07T06:00:00.000040448'\n",
      " '1981-06-07T12:00:00.000040448' '1981-06-07T18:00:00.000040448'\n",
      " '1981-06-08T00:00:00.000040448' '1981-06-08T06:00:00.000040448'\n",
      " '1981-06-08T12:00:00.000040448' '1981-06-08T18:00:00.000040448'\n",
      " '1981-06-09T00:00:00.000040448' '1981-06-09T06:00:00.000040448']\n",
      "['1981-06-15T00:00:00.000040448' '1981-06-15T06:00:00.000040448'\n",
      " '1981-06-15T12:00:00.000040448' '1981-06-15T18:00:00.000040448'\n",
      " '1981-06-16T00:00:00.000040448' '1981-06-16T06:00:00.000040448'\n",
      " '1981-06-16T12:00:00.000040448' '1981-06-16T18:00:00.000040448'\n",
      " '1981-06-17T00:00:00.000040448' '1981-06-17T06:00:00.000040448']\n",
      "['1981-06-25T00:00:00.000040448' '1981-06-25T06:00:00.000040448'\n",
      " '1981-06-25T12:00:00.000040448' '1981-06-25T18:00:00.000040448'\n",
      " '1981-06-26T00:00:00.000040448' '1981-06-26T06:00:00.000040448'\n",
      " '1981-06-26T12:00:00.000040448' '1981-06-26T18:00:00.000040448'\n",
      " '1981-06-27T00:00:00.000040448' '1981-06-27T06:00:00.000040448']\n",
      "['1981-06-27T00:00:00.000040448' '1981-06-27T06:00:00.000040448'\n",
      " '1981-06-27T12:00:00.000040448' '1981-06-27T18:00:00.000040448'\n",
      " '1981-06-28T00:00:00.000040448' '1981-06-28T06:00:00.000040448'\n",
      " '1981-06-28T12:00:00.000040448' '1981-06-28T18:00:00.000040448'\n",
      " '1981-06-29T00:00:00.000040448' '1981-06-29T06:00:00.000040448']\n",
      "['1981-06-28T18:00:00.000040448' '1981-06-29T00:00:00.000040448'\n",
      " '1981-06-29T06:00:00.000040448' '1981-06-29T12:00:00.000040448'\n",
      " '1981-06-29T18:00:00.000040448' '1981-06-30T00:00:00.000040448'\n",
      " '1981-06-30T06:00:00.000040448' '1981-06-30T12:00:00.000040448'\n",
      " '1981-06-30T18:00:00.000040448' '1981-07-01T00:00:00.000040448']\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(100):\n",
    "    print(ds_80_00_processed_34['time'].isel(storm=i, date_time=np.arange(10)).data)\n",
    "    #6-hourly filtering seems to have worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####   Datetime testing below here   #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = np.empty(3, dtype='datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t[0] = np.datetime64('NaT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
